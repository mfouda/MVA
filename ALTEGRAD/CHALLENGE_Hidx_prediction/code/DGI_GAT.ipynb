{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DGI_GAT.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "SHQ0svZB8408",
        "Vfy3enEk9Cox",
        "r8OvmskJbhO1",
        "9Zy2GfCkiAUT"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHQ0svZB8408"
      },
      "source": [
        "# Download Data and install libraries "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dptxi0Zch_fy"
      },
      "source": [
        "!wget http://glaros.dtc.umn.edu/gkhome/fetch/sw/metis/metis-5.1.0.tar.gz\n",
        "!gunzip metis-5.1.0.tar.gz\n",
        "!tar -xvf metis-5.1.0.tar\n",
        "%cd /content/metis-5.1.0\n",
        "#!makefile config shared=1\n",
        "!make config shared=1\n",
        "!make install\n",
        "%cd .."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwmyJ8bei4Fm"
      },
      "source": [
        "!pip -q install tensorflow-addons\n",
        "!pip -q install stellargraph\n",
        "!pip -q install metis\n",
        "!sudo /sbin/ldconfig -v\n",
        "!export METIS_DLL=/usr/local/lib/libmetis.so"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aISUs7PtNsOb"
      },
      "source": [
        "##download KeyBert author embeddings\n",
        "!gdown --id 134bmQq3LBVB-3o6l67-Bk0p3dGyrzmYi\n",
        "## download author d2v embeddings\n",
        "!gdown --id 1aoVxttcsLHrOKyHxPi9-ePaz-9Ynqa2z\n",
        "## download Altegrad (graph data)\n",
        "!gdown --id \"1zAHe0nRAxzTsrBm3pNfqjQZeHd-uKgTd\"\n",
        "!unzip -q -o \"/content/altegrad-2020.zip\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DTRe2HDOIHc"
      },
      "source": [
        "import networkx as nx\n",
        "\n",
        "import stellargraph as sg\n",
        "from stellargraph import StellarGraph\n",
        "from stellargraph.mapper import ClusterNodeGenerator, FullBatchNodeGenerator\n",
        "from stellargraph.layer import GCN\n",
        "from stellargraph import globalvar\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow_addons.optimizers import Lookahead,NovoGrad\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import model_selection\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vfy3enEk9Cox"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "b-yI8gBDi97M",
        "outputId": "2cd1ebf8-9006-4150-e793-17dabf3a8417"
      },
      "source": [
        "## use keybert embeddings\n",
        "pd_embeds = pd.read_csv(\"author_embeds_bert768.csv\",compression='gzip')\n",
        "\n",
        "## if you want to use Doc2vec embeddings.\n",
        "#pd_embeds = pd.read_csv(\"author_embeddings_d2v.csv\")\n",
        "#######################\n",
        "\n",
        "pd_embeds.set_index('authorID',inplace=True)\n",
        "G_nx = nx.read_edgelist('collaboration_network.edgelist', delimiter=' ', nodetype=int)\n",
        "G = StellarGraph.from_networkx(G_nx,node_features=pd_embeds, node_type_default=\"author\", edge_type_default=\"cites\")\n",
        "\n",
        "pd_embeds.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>auth_embed_0</th>\n",
              "      <th>auth_embed_1</th>\n",
              "      <th>auth_embed_2</th>\n",
              "      <th>auth_embed_3</th>\n",
              "      <th>auth_embed_4</th>\n",
              "      <th>auth_embed_5</th>\n",
              "      <th>auth_embed_6</th>\n",
              "      <th>auth_embed_7</th>\n",
              "      <th>auth_embed_8</th>\n",
              "      <th>auth_embed_9</th>\n",
              "      <th>auth_embed_10</th>\n",
              "      <th>auth_embed_11</th>\n",
              "      <th>auth_embed_12</th>\n",
              "      <th>auth_embed_13</th>\n",
              "      <th>auth_embed_14</th>\n",
              "      <th>auth_embed_15</th>\n",
              "      <th>auth_embed_16</th>\n",
              "      <th>auth_embed_17</th>\n",
              "      <th>auth_embed_18</th>\n",
              "      <th>auth_embed_19</th>\n",
              "      <th>auth_embed_20</th>\n",
              "      <th>auth_embed_21</th>\n",
              "      <th>auth_embed_22</th>\n",
              "      <th>auth_embed_23</th>\n",
              "      <th>auth_embed_24</th>\n",
              "      <th>auth_embed_25</th>\n",
              "      <th>auth_embed_26</th>\n",
              "      <th>auth_embed_27</th>\n",
              "      <th>auth_embed_28</th>\n",
              "      <th>auth_embed_29</th>\n",
              "      <th>auth_embed_30</th>\n",
              "      <th>auth_embed_31</th>\n",
              "      <th>auth_embed_32</th>\n",
              "      <th>auth_embed_33</th>\n",
              "      <th>auth_embed_34</th>\n",
              "      <th>auth_embed_35</th>\n",
              "      <th>auth_embed_36</th>\n",
              "      <th>auth_embed_37</th>\n",
              "      <th>auth_embed_38</th>\n",
              "      <th>auth_embed_39</th>\n",
              "      <th>...</th>\n",
              "      <th>auth_embed_728</th>\n",
              "      <th>auth_embed_729</th>\n",
              "      <th>auth_embed_730</th>\n",
              "      <th>auth_embed_731</th>\n",
              "      <th>auth_embed_732</th>\n",
              "      <th>auth_embed_733</th>\n",
              "      <th>auth_embed_734</th>\n",
              "      <th>auth_embed_735</th>\n",
              "      <th>auth_embed_736</th>\n",
              "      <th>auth_embed_737</th>\n",
              "      <th>auth_embed_738</th>\n",
              "      <th>auth_embed_739</th>\n",
              "      <th>auth_embed_740</th>\n",
              "      <th>auth_embed_741</th>\n",
              "      <th>auth_embed_742</th>\n",
              "      <th>auth_embed_743</th>\n",
              "      <th>auth_embed_744</th>\n",
              "      <th>auth_embed_745</th>\n",
              "      <th>auth_embed_746</th>\n",
              "      <th>auth_embed_747</th>\n",
              "      <th>auth_embed_748</th>\n",
              "      <th>auth_embed_749</th>\n",
              "      <th>auth_embed_750</th>\n",
              "      <th>auth_embed_751</th>\n",
              "      <th>auth_embed_752</th>\n",
              "      <th>auth_embed_753</th>\n",
              "      <th>auth_embed_754</th>\n",
              "      <th>auth_embed_755</th>\n",
              "      <th>auth_embed_756</th>\n",
              "      <th>auth_embed_757</th>\n",
              "      <th>auth_embed_758</th>\n",
              "      <th>auth_embed_759</th>\n",
              "      <th>auth_embed_760</th>\n",
              "      <th>auth_embed_761</th>\n",
              "      <th>auth_embed_762</th>\n",
              "      <th>auth_embed_763</th>\n",
              "      <th>auth_embed_764</th>\n",
              "      <th>auth_embed_765</th>\n",
              "      <th>auth_embed_766</th>\n",
              "      <th>auth_embed_767</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>authorID</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2108281488</th>\n",
              "      <td>-0.173046</td>\n",
              "      <td>0.393209</td>\n",
              "      <td>1.831028</td>\n",
              "      <td>0.371120</td>\n",
              "      <td>0.242482</td>\n",
              "      <td>-0.495411</td>\n",
              "      <td>-0.726133</td>\n",
              "      <td>0.042257</td>\n",
              "      <td>0.460925</td>\n",
              "      <td>-0.613946</td>\n",
              "      <td>-0.336119</td>\n",
              "      <td>0.582738</td>\n",
              "      <td>-0.155096</td>\n",
              "      <td>0.496062</td>\n",
              "      <td>-0.131147</td>\n",
              "      <td>0.084022</td>\n",
              "      <td>-1.124881</td>\n",
              "      <td>-0.140933</td>\n",
              "      <td>-0.074926</td>\n",
              "      <td>0.084837</td>\n",
              "      <td>-0.253605</td>\n",
              "      <td>-0.096266</td>\n",
              "      <td>-0.329991</td>\n",
              "      <td>0.012299</td>\n",
              "      <td>0.851879</td>\n",
              "      <td>-0.130658</td>\n",
              "      <td>-0.248079</td>\n",
              "      <td>0.121888</td>\n",
              "      <td>-0.085081</td>\n",
              "      <td>-0.159354</td>\n",
              "      <td>-0.362353</td>\n",
              "      <td>0.105696</td>\n",
              "      <td>-0.159798</td>\n",
              "      <td>-0.924181</td>\n",
              "      <td>-0.654802</td>\n",
              "      <td>0.344883</td>\n",
              "      <td>0.286669</td>\n",
              "      <td>-0.077704</td>\n",
              "      <td>0.173877</td>\n",
              "      <td>0.154905</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.598010</td>\n",
              "      <td>-0.826115</td>\n",
              "      <td>1.719798</td>\n",
              "      <td>-0.761064</td>\n",
              "      <td>0.179567</td>\n",
              "      <td>0.640446</td>\n",
              "      <td>0.892722</td>\n",
              "      <td>0.733161</td>\n",
              "      <td>0.066956</td>\n",
              "      <td>0.232464</td>\n",
              "      <td>-0.026667</td>\n",
              "      <td>-0.693748</td>\n",
              "      <td>-0.937065</td>\n",
              "      <td>0.354889</td>\n",
              "      <td>-0.028239</td>\n",
              "      <td>0.696455</td>\n",
              "      <td>-0.446061</td>\n",
              "      <td>0.024826</td>\n",
              "      <td>0.676388</td>\n",
              "      <td>0.156983</td>\n",
              "      <td>-0.157734</td>\n",
              "      <td>0.097707</td>\n",
              "      <td>0.034603</td>\n",
              "      <td>-0.720089</td>\n",
              "      <td>0.414393</td>\n",
              "      <td>0.288075</td>\n",
              "      <td>-0.383244</td>\n",
              "      <td>0.190391</td>\n",
              "      <td>-0.330003</td>\n",
              "      <td>0.187892</td>\n",
              "      <td>-0.281891</td>\n",
              "      <td>-0.852649</td>\n",
              "      <td>-0.476081</td>\n",
              "      <td>-0.633179</td>\n",
              "      <td>-0.690824</td>\n",
              "      <td>-0.123956</td>\n",
              "      <td>0.156237</td>\n",
              "      <td>-0.852115</td>\n",
              "      <td>-1.495437</td>\n",
              "      <td>0.033236</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2595221017</th>\n",
              "      <td>-0.197502</td>\n",
              "      <td>-0.064884</td>\n",
              "      <td>1.619865</td>\n",
              "      <td>-0.014593</td>\n",
              "      <td>0.792456</td>\n",
              "      <td>-0.864575</td>\n",
              "      <td>-0.037857</td>\n",
              "      <td>-0.165942</td>\n",
              "      <td>0.276517</td>\n",
              "      <td>-0.555294</td>\n",
              "      <td>-0.631644</td>\n",
              "      <td>0.116087</td>\n",
              "      <td>-0.433079</td>\n",
              "      <td>0.563198</td>\n",
              "      <td>-0.049755</td>\n",
              "      <td>0.489740</td>\n",
              "      <td>-0.389780</td>\n",
              "      <td>0.468629</td>\n",
              "      <td>-0.035671</td>\n",
              "      <td>-0.259494</td>\n",
              "      <td>-0.996263</td>\n",
              "      <td>0.508401</td>\n",
              "      <td>0.713443</td>\n",
              "      <td>-0.176310</td>\n",
              "      <td>0.682409</td>\n",
              "      <td>0.020635</td>\n",
              "      <td>-0.100275</td>\n",
              "      <td>0.634538</td>\n",
              "      <td>-0.272070</td>\n",
              "      <td>0.307866</td>\n",
              "      <td>0.021029</td>\n",
              "      <td>-0.364701</td>\n",
              "      <td>0.281459</td>\n",
              "      <td>-0.348118</td>\n",
              "      <td>-0.306025</td>\n",
              "      <td>0.015887</td>\n",
              "      <td>0.131905</td>\n",
              "      <td>-0.432933</td>\n",
              "      <td>0.109667</td>\n",
              "      <td>-0.089357</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.676947</td>\n",
              "      <td>-0.555962</td>\n",
              "      <td>-0.249646</td>\n",
              "      <td>-0.447208</td>\n",
              "      <td>-0.038728</td>\n",
              "      <td>0.235958</td>\n",
              "      <td>0.978269</td>\n",
              "      <td>0.097761</td>\n",
              "      <td>0.015489</td>\n",
              "      <td>-0.308050</td>\n",
              "      <td>-0.176071</td>\n",
              "      <td>-0.521514</td>\n",
              "      <td>-0.589157</td>\n",
              "      <td>-0.882482</td>\n",
              "      <td>-0.884801</td>\n",
              "      <td>0.661087</td>\n",
              "      <td>-0.235123</td>\n",
              "      <td>-0.218746</td>\n",
              "      <td>0.215607</td>\n",
              "      <td>0.418752</td>\n",
              "      <td>-0.721329</td>\n",
              "      <td>0.268213</td>\n",
              "      <td>0.336433</td>\n",
              "      <td>-0.037905</td>\n",
              "      <td>0.255193</td>\n",
              "      <td>0.882667</td>\n",
              "      <td>0.473684</td>\n",
              "      <td>0.300780</td>\n",
              "      <td>-0.686156</td>\n",
              "      <td>-0.092590</td>\n",
              "      <td>-0.650778</td>\n",
              "      <td>-1.114661</td>\n",
              "      <td>-0.039282</td>\n",
              "      <td>-0.118444</td>\n",
              "      <td>-0.244923</td>\n",
              "      <td>-0.153532</td>\n",
              "      <td>0.087446</td>\n",
              "      <td>0.042333</td>\n",
              "      <td>-0.823323</td>\n",
              "      <td>0.500721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2097270384</th>\n",
              "      <td>-0.192959</td>\n",
              "      <td>0.001563</td>\n",
              "      <td>1.654724</td>\n",
              "      <td>0.365408</td>\n",
              "      <td>0.396445</td>\n",
              "      <td>-0.608603</td>\n",
              "      <td>-0.352275</td>\n",
              "      <td>0.338049</td>\n",
              "      <td>-0.120389</td>\n",
              "      <td>-0.371518</td>\n",
              "      <td>-0.578296</td>\n",
              "      <td>0.462430</td>\n",
              "      <td>-0.789411</td>\n",
              "      <td>0.935133</td>\n",
              "      <td>0.549331</td>\n",
              "      <td>0.293660</td>\n",
              "      <td>-1.135520</td>\n",
              "      <td>0.178950</td>\n",
              "      <td>-0.177027</td>\n",
              "      <td>0.015445</td>\n",
              "      <td>-0.037963</td>\n",
              "      <td>-0.215782</td>\n",
              "      <td>-0.749321</td>\n",
              "      <td>0.028821</td>\n",
              "      <td>0.997122</td>\n",
              "      <td>0.002454</td>\n",
              "      <td>-0.392629</td>\n",
              "      <td>-0.169500</td>\n",
              "      <td>-0.804588</td>\n",
              "      <td>-0.054057</td>\n",
              "      <td>-0.848934</td>\n",
              "      <td>0.142752</td>\n",
              "      <td>0.187889</td>\n",
              "      <td>-0.549464</td>\n",
              "      <td>-0.328108</td>\n",
              "      <td>0.217560</td>\n",
              "      <td>-0.149524</td>\n",
              "      <td>-0.380249</td>\n",
              "      <td>0.418266</td>\n",
              "      <td>-0.326569</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.746994</td>\n",
              "      <td>-0.469232</td>\n",
              "      <td>1.150694</td>\n",
              "      <td>-0.748614</td>\n",
              "      <td>0.096924</td>\n",
              "      <td>0.493252</td>\n",
              "      <td>-0.018203</td>\n",
              "      <td>-0.224732</td>\n",
              "      <td>0.192401</td>\n",
              "      <td>0.102611</td>\n",
              "      <td>-0.700352</td>\n",
              "      <td>-0.320948</td>\n",
              "      <td>-0.523485</td>\n",
              "      <td>-0.306408</td>\n",
              "      <td>-0.087058</td>\n",
              "      <td>0.075169</td>\n",
              "      <td>-0.273571</td>\n",
              "      <td>0.307482</td>\n",
              "      <td>1.067765</td>\n",
              "      <td>0.241145</td>\n",
              "      <td>-0.763631</td>\n",
              "      <td>0.658328</td>\n",
              "      <td>-0.081867</td>\n",
              "      <td>-1.038291</td>\n",
              "      <td>0.137976</td>\n",
              "      <td>0.870237</td>\n",
              "      <td>-0.258353</td>\n",
              "      <td>0.226808</td>\n",
              "      <td>-0.747607</td>\n",
              "      <td>-0.322230</td>\n",
              "      <td>-0.310240</td>\n",
              "      <td>-0.333399</td>\n",
              "      <td>-0.976173</td>\n",
              "      <td>0.048537</td>\n",
              "      <td>-0.065136</td>\n",
              "      <td>-0.617560</td>\n",
              "      <td>-0.023539</td>\n",
              "      <td>-0.869433</td>\n",
              "      <td>-0.564192</td>\n",
              "      <td>0.158580</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2147216009</th>\n",
              "      <td>-0.985928</td>\n",
              "      <td>0.539726</td>\n",
              "      <td>0.853956</td>\n",
              "      <td>0.542073</td>\n",
              "      <td>0.184018</td>\n",
              "      <td>0.101193</td>\n",
              "      <td>-0.681880</td>\n",
              "      <td>-0.936817</td>\n",
              "      <td>0.359112</td>\n",
              "      <td>-0.898177</td>\n",
              "      <td>-0.627917</td>\n",
              "      <td>0.304435</td>\n",
              "      <td>0.010130</td>\n",
              "      <td>-0.132877</td>\n",
              "      <td>0.081372</td>\n",
              "      <td>0.354781</td>\n",
              "      <td>-0.613645</td>\n",
              "      <td>-0.981481</td>\n",
              "      <td>-0.308267</td>\n",
              "      <td>0.090581</td>\n",
              "      <td>0.293163</td>\n",
              "      <td>0.272516</td>\n",
              "      <td>-0.246858</td>\n",
              "      <td>-0.738789</td>\n",
              "      <td>0.771049</td>\n",
              "      <td>0.276306</td>\n",
              "      <td>-0.216015</td>\n",
              "      <td>0.419478</td>\n",
              "      <td>-1.267514</td>\n",
              "      <td>0.540901</td>\n",
              "      <td>-0.663147</td>\n",
              "      <td>0.165398</td>\n",
              "      <td>-0.097579</td>\n",
              "      <td>-0.501034</td>\n",
              "      <td>0.048533</td>\n",
              "      <td>0.016264</td>\n",
              "      <td>0.941655</td>\n",
              "      <td>-0.000413</td>\n",
              "      <td>0.037218</td>\n",
              "      <td>0.035471</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.692499</td>\n",
              "      <td>-0.722468</td>\n",
              "      <td>0.300047</td>\n",
              "      <td>-0.449808</td>\n",
              "      <td>0.036611</td>\n",
              "      <td>0.081426</td>\n",
              "      <td>-0.281566</td>\n",
              "      <td>0.115229</td>\n",
              "      <td>0.877577</td>\n",
              "      <td>0.291445</td>\n",
              "      <td>-0.065399</td>\n",
              "      <td>-0.273032</td>\n",
              "      <td>-1.105204</td>\n",
              "      <td>0.804583</td>\n",
              "      <td>0.267831</td>\n",
              "      <td>0.159318</td>\n",
              "      <td>-0.255606</td>\n",
              "      <td>-0.131470</td>\n",
              "      <td>-0.009059</td>\n",
              "      <td>-0.496360</td>\n",
              "      <td>-0.121599</td>\n",
              "      <td>0.144621</td>\n",
              "      <td>-1.098639</td>\n",
              "      <td>0.718992</td>\n",
              "      <td>0.271880</td>\n",
              "      <td>1.646500</td>\n",
              "      <td>-0.806599</td>\n",
              "      <td>-0.129040</td>\n",
              "      <td>-0.228818</td>\n",
              "      <td>-0.575628</td>\n",
              "      <td>0.479092</td>\n",
              "      <td>-0.161632</td>\n",
              "      <td>0.293961</td>\n",
              "      <td>-0.819277</td>\n",
              "      <td>0.145908</td>\n",
              "      <td>-0.720663</td>\n",
              "      <td>-0.368299</td>\n",
              "      <td>-0.351254</td>\n",
              "      <td>-0.927054</td>\n",
              "      <td>1.032906</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2401159163</th>\n",
              "      <td>-0.602504</td>\n",
              "      <td>0.842025</td>\n",
              "      <td>1.143953</td>\n",
              "      <td>0.211573</td>\n",
              "      <td>0.573727</td>\n",
              "      <td>-0.094351</td>\n",
              "      <td>0.119689</td>\n",
              "      <td>-0.125083</td>\n",
              "      <td>-0.191311</td>\n",
              "      <td>-0.191083</td>\n",
              "      <td>-0.677808</td>\n",
              "      <td>0.893347</td>\n",
              "      <td>-0.273810</td>\n",
              "      <td>0.321763</td>\n",
              "      <td>-0.077764</td>\n",
              "      <td>0.149002</td>\n",
              "      <td>-1.016810</td>\n",
              "      <td>-0.284966</td>\n",
              "      <td>0.554528</td>\n",
              "      <td>-0.302526</td>\n",
              "      <td>0.170836</td>\n",
              "      <td>0.022091</td>\n",
              "      <td>-0.655019</td>\n",
              "      <td>0.637926</td>\n",
              "      <td>1.525833</td>\n",
              "      <td>0.279931</td>\n",
              "      <td>-0.074624</td>\n",
              "      <td>-0.318752</td>\n",
              "      <td>-0.287692</td>\n",
              "      <td>0.085173</td>\n",
              "      <td>-0.545865</td>\n",
              "      <td>0.102945</td>\n",
              "      <td>-0.051201</td>\n",
              "      <td>-0.757899</td>\n",
              "      <td>-0.428936</td>\n",
              "      <td>0.025139</td>\n",
              "      <td>0.184436</td>\n",
              "      <td>-0.377132</td>\n",
              "      <td>-0.177416</td>\n",
              "      <td>0.009420</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.674457</td>\n",
              "      <td>-0.219443</td>\n",
              "      <td>0.712441</td>\n",
              "      <td>-0.082834</td>\n",
              "      <td>0.040605</td>\n",
              "      <td>0.708211</td>\n",
              "      <td>0.679830</td>\n",
              "      <td>0.012126</td>\n",
              "      <td>-0.881064</td>\n",
              "      <td>0.008703</td>\n",
              "      <td>-0.282096</td>\n",
              "      <td>-0.813060</td>\n",
              "      <td>-0.317125</td>\n",
              "      <td>-0.112103</td>\n",
              "      <td>-0.149589</td>\n",
              "      <td>0.438785</td>\n",
              "      <td>-0.503514</td>\n",
              "      <td>0.561765</td>\n",
              "      <td>0.351165</td>\n",
              "      <td>0.435660</td>\n",
              "      <td>-0.607811</td>\n",
              "      <td>0.419245</td>\n",
              "      <td>-0.326215</td>\n",
              "      <td>0.219162</td>\n",
              "      <td>0.130067</td>\n",
              "      <td>0.302595</td>\n",
              "      <td>0.008445</td>\n",
              "      <td>-0.315605</td>\n",
              "      <td>-0.726949</td>\n",
              "      <td>-0.197428</td>\n",
              "      <td>-0.324734</td>\n",
              "      <td>-0.849457</td>\n",
              "      <td>-0.375139</td>\n",
              "      <td>-0.015620</td>\n",
              "      <td>-0.490683</td>\n",
              "      <td>0.238964</td>\n",
              "      <td>-0.036214</td>\n",
              "      <td>-0.677901</td>\n",
              "      <td>-0.607660</td>\n",
              "      <td>0.642289</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 768 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            auth_embed_0  auth_embed_1  ...  auth_embed_766  auth_embed_767\n",
              "authorID                                ...                                \n",
              "2108281488     -0.173046      0.393209  ...       -1.495437        0.033236\n",
              "2595221017     -0.197502     -0.064884  ...       -0.823323        0.500721\n",
              "2097270384     -0.192959      0.001563  ...       -0.564192        0.158580\n",
              "2147216009     -0.985928      0.539726  ...       -0.927054        1.032906\n",
              "2401159163     -0.602504      0.842025  ...       -0.607660        0.642289\n",
              "\n",
              "[5 rows x 768 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2J2QoOU6PL92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "outputId": "8c7158e8-b9b5-429c-c7e0-22e30c2b8eea"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "targets = pd.read_csv('train.csv', dtype={'authorID': np.int64, 'h_index': np.float32})\n",
        "\n",
        "targets.set_index(\"authorID\",inplace=True)\n",
        "# Use scikit-learn to compute training and test sets\n",
        "train_targets, test_targets = model_selection.train_test_split(targets, test_size=0.05)\n",
        "\n",
        "from scipy.stats import boxcox\n",
        "y = targets[\"h_index\"].values\n",
        "y_sc,ld= boxcox(y)\n",
        "targets_sc = pd.DataFrame(y_sc,index=targets.index)\n",
        "train_targets_sc, test_targets_sc = model_selection.train_test_split(targets_sc, test_size=0.2)\n",
        "plt.hist(targets_sc.values)\n",
        "\n",
        "#plt.hist(targets.values)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([3931., 2805., 2104., 4483., 2677., 3257., 2180., 1202.,  458.,\n",
              "          27.]),\n",
              " array([0.        , 0.42177835, 0.8435567 , 1.2653351 , 1.6871134 ,\n",
              "        2.1088917 , 2.5306702 , 2.9524484 , 3.3742268 , 3.796005  ,\n",
              "        4.2177835 ], dtype=float32),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "id": "KKQtX1cvykBN",
        "outputId": "6a7e7b89-742d-4114-91de-2378afa1234b"
      },
      "source": [
        "targets.shape\n",
        "plt.hist(targets.values,bins=20)\n",
        "targets.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "h_index\n",
              "1.0        3931\n",
              "2.0        2805\n",
              "3.0        2104\n",
              "4.0        1780\n",
              "5.0        1488\n",
              "           ... \n",
              "92.0          1\n",
              "91.0          1\n",
              "89.0          1\n",
              "88.0          1\n",
              "163.0         1\n",
              "Length: 107, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "St4Cc_ehXDjW"
      },
      "source": [
        "## define custom losses for training \n",
        "\n",
        "def real_mae(y_true,y_pred):\n",
        "      return K.mean(K.abs(tf.math.round(y_pred)-y_true))\n",
        "\n",
        "def inv_boxcox(y):\n",
        "  return K.exp(K.log(ld * y + 1 )/ld)\n",
        "\n",
        "def scaled_mae(y_true,y_pred):\n",
        "  \"\"\" To use with scaled targets\"\"\"\n",
        "  y_true = inv_boxcox(y_true)\n",
        "  y_pred = inv_boxcox(y_pred)\n",
        "  return real_mae(y_true,y_pred)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTfYSeh3fABR",
        "outputId": "796266a2-2a06-4978-c841-1a80a6cf8bfe"
      },
      "source": [
        "# We are going to use the METIS clustering algorith,\n",
        "\n",
        "\"\"\" Here we break the graph into equal sized subgraphs \n",
        "\n",
        "This allows for batch training on the graph and avoids memory overflows\"\"\"\n",
        "\n",
        "import metis\n",
        "\n",
        "number_of_clusters = 5  # the number of clusters/subgraphs\n",
        "clusters_per_batch = 1  # combine two cluster per batch\n",
        "node_ids = np.array(G.nodes())\n",
        "\n",
        "###############\n",
        "\n",
        "lil_adj = G.to_adjacency_matrix().tolil()\n",
        "adjlist = [tuple(neighbours) for neighbours in lil_adj.rows]\n",
        "\n",
        "edgecuts, parts = metis.part_graph(adjlist, number_of_clusters)\n",
        "parts = np.array(parts)\n",
        "clusters = []\n",
        "cluster_ids = np.unique(parts)\n",
        "for cluster_id in cluster_ids:\n",
        "    mask = np.where(parts == cluster_id)\n",
        "    clusters.append(node_ids[mask])\n",
        "\n",
        "###########\n",
        "\n",
        "\n",
        "def get_cluster_gen():\n",
        "  cluster_generator = ClusterNodeGenerator(G, clusters=clusters, q=clusters_per_batch, lam=0.1)\n",
        "  return cluster_generator\n",
        "\n",
        "def get_full_batch_gen(model=\"gat\"):\n",
        "  full_batch_generator = FullBatchNodeGenerator(G, method=model)\n",
        "  return full_batch_generator\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Graph clustering using the METIS algorithm.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8OvmskJbhO1"
      },
      "source": [
        "# Simple GNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "id": "J10ZjatVbdkT",
        "outputId": "6723a999-936f-4c5e-eae5-6e221efa93ea"
      },
      "source": [
        "def create_model(generator,mtype=\"gcn\"):\n",
        "\n",
        "  \n",
        "  if mtype == \"gcn\":\n",
        "    gnn = GCN(\n",
        "    layer_sizes=[124, 124], activations=[\"elu\", \"elu\"], generator=generator, dropout=0.5)\n",
        "\n",
        "  if mtype == \"gat\":\n",
        "    gnn = sg.layer.GAT(layer_sizes=[128,128], activations=['elu','elu'],attn_heads=2,in_dropout=0.5,\n",
        "        attn_dropout=0.5,generator=generator)\n",
        "  \n",
        "\n",
        "  x_inp, x_out = gnn.in_out_tensors() # create the input and output TensorFlow tensors\n",
        "\n",
        "  # use TensorFlow Keras to add a layer to compute the (one-hot) predictions\n",
        "  predictions = tf.keras.layers.Dense(units=1, activation=\"relu\")(x_out)\n",
        "\n",
        "  # use the input and output tensors to create a TensorFlow Keras model\n",
        "  model = tf.keras.Model(inputs=x_inp, outputs=predictions)\n",
        "\n",
        "\n",
        "  ### Wrap the model with tf function to avoid OOM\n",
        "  model.call = tf.function(model.call)\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "def fit_model(mtype='gcn',y_normalized=True,full_batch=False):\n",
        "\n",
        "  \n",
        "\n",
        "  if full_batch: generator = full_batch_generator ### train with the full graph as a batch\n",
        "  else : generator = cluster_generator ### break the graph into smaller mini batches\n",
        "\n",
        "  model = create_model(generator,mtype)\n",
        "  optimizer = Lookahead(NovoGrad())\n",
        "  #optimizer = Lookahead(Adam())\n",
        "  #optimizer = Adam(lr=1e-3)\n",
        "\n",
        "  if y_normalized : \n",
        "    # train with log normalized targets\n",
        "    model.compile(optimizer, loss=\"mae\",metrics=[scaled_mae])\n",
        "    model.fit(generator.flow(train_targets_sc.index, train_targets_sc),validation_data = generator.flow(test_targets_sc.index, test_targets_sc), epochs=500)\n",
        "  \n",
        "  else : \n",
        "\n",
        "    model.compile(optimizer, loss=\"mae\",metrics=[real_mae])\n",
        "    model.fit(generator.flow(train_targets.index, train_targets),validation_data = generator.flow(test_targets.index, test_targets), epochs=500)\n",
        "  \n",
        "\n",
        "  return model\n",
        "\n",
        "trained_model = fit_model(y_normalized=True,full_batch=True,mtype='gat')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-73-4e9ade009ca3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mtrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_normalized\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfull_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gcn'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-73-4e9ade009ca3>\u001b[0m in \u001b[0;36mfit_model\u001b[0;34m(mtype, y_normalized, full_batch)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m# train with log normalized targets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mae\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mscaled_mae\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_targets_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_targets_sc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_targets_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_targets_sc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m   \u001b[0;32melse\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    889\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m:  Cannot use GPU when output.shape[1] * nnz(a) > 2^31\n\t [[{{node model_32/StatefulPartitionedCall/graph_convolution_18/SparseTensorDenseMatMul/SparseTensorDenseMatMul}}]] [Op:__inference_train_function_111619]\n\nFunction call stack:\ntrain_function\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Zy2GfCkiAUT"
      },
      "source": [
        "# Pretraining with DGI:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WecCbjVIiC2b"
      },
      "source": [
        "from stellargraph.mapper import (\n",
        "    CorruptedGenerator,\n",
        "    FullBatchNodeGenerator,\n",
        "    GraphSAGENodeGenerator,\n",
        "    HinSAGENodeGenerator,\n",
        "    ClusterNodeGenerator,\n",
        ")\n",
        "from stellargraph import StellarGraph\n",
        "from stellargraph.layer import GCN, DeepGraphInfomax, GraphSAGE, GAT, APPNP, HinSAGE\n",
        "\n",
        "from stellargraph import datasets\n",
        "from stellargraph.utils import plot_history\n",
        "\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.manifold import TSNE\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TY2hsVOiJfD"
      },
      "source": [
        "fullbatch_generator = FullBatchNodeGenerator(G, sparse=True)\n",
        "pretrained_model = GAT(layer_sizes=[64,64,64], activations=[\"relu\",\"relu\",\"relu\"],attn_heads=3,attn_dropout=0.1,in_dropout=0.1,generator=fullbatch_generator)\n",
        "\n",
        "corrupted_generator = CorruptedGenerator(fullbatch_generator)\n",
        "gen = corrupted_generator.flow(G.nodes())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGN6FOL8inIw"
      },
      "source": [
        "infomax = DeepGraphInfomax(pretrained_model, corrupted_generator)\n",
        "x_in, x_out = infomax.in_out_tensors()\n",
        "\n",
        "model = Model(inputs=x_in, outputs=x_out)\n",
        "model.compile(loss=tf.nn.sigmoid_cross_entropy_with_logits, optimizer=Adam(lr=1e-3),metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdqKUdOxiv2Z"
      },
      "source": [
        "es = EarlyStopping(monitor=\"loss\", min_delta=1e-2, patience=20)\n",
        "history = model.fit(gen, epochs=100, verbose=2, callbacks=[es])\n",
        "plot_history(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XUIaQtnlju-"
      },
      "source": [
        "train_gen = fullbatch_generator.flow(train_targets.index, train_targets)\n",
        "test_gen = fullbatch_generator.flow(test_targets.index, test_targets)\n",
        "\n",
        "pretrained_x_in, pretrained_x_out = pretrained_model.in_out_tensors()\n",
        "tuned_predictions = tf.keras.layers.Dense(units=1, activation=\"relu\")(pretrained_x_out)\n",
        "tuned_model = Model(inputs=pretrained_x_in, outputs=tuned_predictions)\n",
        "\n",
        "tuned_model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.01), loss=\"mae\")\n",
        "tuned_history = tuned_model.fit(\n",
        "    train_gen,\n",
        "    epochs=100,\n",
        "    verbose=2,\n",
        "    validation_data=test_gen,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}