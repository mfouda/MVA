{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Deep__APPNP.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Dq4cCBqDx6bc",
        "3BG_d3FcEpmQ"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dq4cCBqDx6bc"
      },
      "source": [
        "# Downloads + Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3yVF9zjnk2V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3ffe703-df62-4a1e-ed3e-53719dde5b27"
      },
      "source": [
        "!pip -q install tensorflow_addons\n",
        "!pip install optuna\n",
        "!pip -q install stellargraph\n",
        "!pip install -q neptune-client==0.4.132 neptune-contrib[monitoring]==0.25.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: optuna in /usr/local/lib/python3.7/dist-packages (2.5.0)\n",
            "Requirement already satisfied: cmaes>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (0.8.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from optuna) (1.19.5)\n",
            "Requirement already satisfied: alembic in /usr/local/lib/python3.7/dist-packages (from optuna) (1.5.5)\n",
            "Requirement already satisfied: scipy!=1.4.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (20.9)\n",
            "Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.3.23)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from optuna) (1.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from optuna) (4.41.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.7/dist-packages (from optuna) (4.7.2)\n",
            "Requirement already satisfied: cliff in /usr/local/lib/python3.7/dist-packages (from optuna) (3.7.0)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.7/dist-packages (from alembic->optuna) (1.1.4)\n",
            "Requirement already satisfied: python-editor>=0.3 in /usr/local/lib/python3.7/dist-packages (from alembic->optuna) (1.0.4)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from alembic->optuna) (2.8.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->optuna) (2.4.7)\n",
            "Requirement already satisfied: PyYAML>=3.12 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (3.13)\n",
            "Requirement already satisfied: stevedore>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (3.3.0)\n",
            "Requirement already satisfied: cmd2>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (1.5.0)\n",
            "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (5.5.1)\n",
            "Requirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (2.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic->optuna) (1.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil->alembic->optuna) (1.15.0)\n",
            "Requirement already satisfied: importlib-metadata>=1.7.0; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from stevedore>=2.0.1->cliff->optuna) (3.7.0)\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (20.3.0)\n",
            "Requirement already satisfied: pyperclip>=1.6 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (1.8.2)\n",
            "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\n",
            "Requirement already satisfied: colorama>=0.3.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (0.4.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from PrettyTable>=0.7.2->cliff->optuna) (54.0.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=1.7.0; python_version < \"3.8\"->stevedore>=2.0.1->cliff->optuna) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=1.7.0; python_version < \"3.8\"->stevedore>=2.0.1->cliff->optuna) (3.4.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0BRiwSK4539",
        "outputId": "273b7d55-cfbb-4975-db69-8b07f453a07f"
      },
      "source": [
        "## download deepwalk embeds \n",
        "!gdown --id 1isrA9Q_ivusLhfpgncbS5g51c8lX5XzK\n",
        "## download author embeddings (avg,std,minmax)\n",
        "!wget -c https://icecube-eu-290.icedrive.io/download?p=bZ_sA1Z5mCom.j0ZWsBXrB4ZaK3TBlqQzZj9tp7mOdjD6HTKsIng0zmgrpkDUo_Wcfwh9y5sRy249IP3Ny1RIARlGP7WGq6xziTgGlQU0OKTDNaz_jXpfYbTGm6Rawau9sM.jie8vpT.SB.MG_DWASLCWDvPByqJ_E8urknzVjiCh1LNKqtUdxFh8qwlds6L09KZG2g3J0PwD2ec2y0Djg-- -O  \"author_embeds_full2.csv\"\n",
        "## download chtayra embedding:\n",
        "!gdown --id 1HRM1b7-CcE4G6zadLS-LZPaCWhwhoFWi\n",
        "# # ## download node cluster data\n",
        "!gdown --id 1-1MsoyleTbh1ns5y0OOz91AJc0vJA5BX\n",
        "\n",
        "## download competition data\n",
        "!gdown --id \"1zAHe0nRAxzTsrBm3pNfqjQZeHd-uKgTd\"\n",
        "!unzip -o -q \"/content/altegrad-2020.zip\"\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1isrA9Q_ivusLhfpgncbS5g51c8lX5XzK\n",
            "To: /content/colab_net_deepwalk_embeddings.txt\n",
            "167MB [00:01, 118MB/s]\n",
            "--2021-03-03 18:09:26--  https://icecube-eu-290.icedrive.io/download?p=bZ_sA1Z5mCom.j0ZWsBXrB4ZaK3TBlqQzZj9tp7mOdjD6HTKsIng0zmgrpkDUo_Wcfwh9y5sRy249IP3Ny1RIARlGP7WGq6xziTgGlQU0OKTDNaz_jXpfYbTGm6Rawau9sM.jie8vpT.SB.MG_DWASLCWDvPByqJ_E8urknzVjiCh1LNKqtUdxFh8qwlds6L09KZG2g3J0PwD2ec2y0Djg--\n",
            "Resolving icecube-eu-290.icedrive.io (icecube-eu-290.icedrive.io)... 46.165.242.15\n",
            "Connecting to icecube-eu-290.icedrive.io (icecube-eu-290.icedrive.io)|46.165.242.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable\n",
            "\n",
            "    The file is already fully retrieved; nothing to do.\n",
            "\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1HRM1b7-CcE4G6zadLS-LZPaCWhwhoFWi\n",
            "To: /content/author_keywds_AHMED.csv\n",
            "283MB [00:01, 180MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-1MsoyleTbh1ns5y0OOz91AJc0vJA5BX\n",
            "To: /content/node_cluster_data.csv\n",
            "62.7MB [00:00, 151MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1zAHe0nRAxzTsrBm3pNfqjQZeHd-uKgTd\n",
            "To: /content/altegrad-2020.zip\n",
            "19.8MB [00:00, 75.2MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ik0uJUqncg6L"
      },
      "source": [
        "import tensorflow_addons as tfa\n",
        "from tensorflow_addons.optimizers import NovoGrad,Lookahead\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense,Dropout , BatchNormalization ,LayerNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping,ReduceLROnPlateau\n",
        "from tensorflow.nn import leaky_relu\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "import stellargraph as sg\n",
        "from stellargraph.mapper import (\n",
        "    CorruptedGenerator,\n",
        "    FullBatchNodeGenerator,\n",
        "    GraphSAGENodeGenerator,\n",
        "    HinSAGENodeGenerator,\n",
        "    ClusterNodeGenerator,\n",
        ")\n",
        "from stellargraph import StellarGraph\n",
        "from stellargraph.layer import GCN, DeepGraphInfomax, GraphSAGE, GAT, APPNP, HinSAGE\n",
        "\n",
        "\n",
        "from stellargraph.layer.ppnp import PPNP\n",
        "from stellargraph.layer.appnp import APPNP\n",
        "\n",
        "from stellargraph import datasets\n",
        "from stellargraph.utils import plot_history\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler,RobustScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from scipy.stats import boxcox\n",
        "import matplotlib.pyplot as plt \n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import model_selection\n",
        "\n",
        "import networkx as nx\n",
        "import optuna\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BG_d3FcEpmQ"
      },
      "source": [
        "# Load data + Pre processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "w02WSBQu4CF_",
        "outputId": "92e6ae42-f55f-4785-8195-1480a7cff679"
      },
      "source": [
        "author_embeds = pd.read_csv(\"author_embeds_full2.csv\")\n",
        "author_embeds.rename(columns = {\"Unnamed: 0\":\"authorID\"},inplace=True)\n",
        "author_embeds.set_index(author_embeds['authorID'],inplace=True)\n",
        "author_embeds.drop(columns=['authorID'],inplace=True)\n",
        "author_embeds.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>avg 0</th>\n",
              "      <th>avg 1</th>\n",
              "      <th>avg 2</th>\n",
              "      <th>avg 3</th>\n",
              "      <th>avg 4</th>\n",
              "      <th>avg 5</th>\n",
              "      <th>avg 6</th>\n",
              "      <th>avg 7</th>\n",
              "      <th>avg 8</th>\n",
              "      <th>avg 9</th>\n",
              "      <th>avg 10</th>\n",
              "      <th>avg 11</th>\n",
              "      <th>avg 12</th>\n",
              "      <th>avg 13</th>\n",
              "      <th>avg 14</th>\n",
              "      <th>avg 15</th>\n",
              "      <th>avg 16</th>\n",
              "      <th>avg 17</th>\n",
              "      <th>avg 18</th>\n",
              "      <th>avg 19</th>\n",
              "      <th>avg 20</th>\n",
              "      <th>avg 21</th>\n",
              "      <th>avg 22</th>\n",
              "      <th>avg 23</th>\n",
              "      <th>avg 24</th>\n",
              "      <th>avg 25</th>\n",
              "      <th>avg 26</th>\n",
              "      <th>avg 27</th>\n",
              "      <th>avg 28</th>\n",
              "      <th>avg 29</th>\n",
              "      <th>avg 30</th>\n",
              "      <th>avg 31</th>\n",
              "      <th>avg 32</th>\n",
              "      <th>avg 33</th>\n",
              "      <th>avg 34</th>\n",
              "      <th>avg 35</th>\n",
              "      <th>avg 36</th>\n",
              "      <th>avg 37</th>\n",
              "      <th>avg 38</th>\n",
              "      <th>avg 39</th>\n",
              "      <th>...</th>\n",
              "      <th>max 217</th>\n",
              "      <th>max 218</th>\n",
              "      <th>max 219</th>\n",
              "      <th>max 220</th>\n",
              "      <th>max 221</th>\n",
              "      <th>max 222</th>\n",
              "      <th>max 223</th>\n",
              "      <th>max 224</th>\n",
              "      <th>max 225</th>\n",
              "      <th>max 226</th>\n",
              "      <th>max 227</th>\n",
              "      <th>max 228</th>\n",
              "      <th>max 229</th>\n",
              "      <th>max 230</th>\n",
              "      <th>max 231</th>\n",
              "      <th>max 232</th>\n",
              "      <th>max 233</th>\n",
              "      <th>max 234</th>\n",
              "      <th>max 235</th>\n",
              "      <th>max 236</th>\n",
              "      <th>max 237</th>\n",
              "      <th>max 238</th>\n",
              "      <th>max 239</th>\n",
              "      <th>max 240</th>\n",
              "      <th>max 241</th>\n",
              "      <th>max 242</th>\n",
              "      <th>max 243</th>\n",
              "      <th>max 244</th>\n",
              "      <th>max 245</th>\n",
              "      <th>max 246</th>\n",
              "      <th>max 247</th>\n",
              "      <th>max 248</th>\n",
              "      <th>max 249</th>\n",
              "      <th>max 250</th>\n",
              "      <th>max 251</th>\n",
              "      <th>max 252</th>\n",
              "      <th>max 253</th>\n",
              "      <th>max 254</th>\n",
              "      <th>max 255</th>\n",
              "      <th>n_papers</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>authorID</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1036332</th>\n",
              "      <td>-0.670915</td>\n",
              "      <td>-0.359066</td>\n",
              "      <td>0.032644</td>\n",
              "      <td>0.001299</td>\n",
              "      <td>0.516241</td>\n",
              "      <td>-0.401442</td>\n",
              "      <td>-0.266719</td>\n",
              "      <td>-0.309738</td>\n",
              "      <td>-0.317130</td>\n",
              "      <td>0.317681</td>\n",
              "      <td>-0.330467</td>\n",
              "      <td>0.612775</td>\n",
              "      <td>-0.021864</td>\n",
              "      <td>-0.729525</td>\n",
              "      <td>-0.032873</td>\n",
              "      <td>0.296621</td>\n",
              "      <td>-0.117286</td>\n",
              "      <td>-0.256655</td>\n",
              "      <td>0.706496</td>\n",
              "      <td>-0.183294</td>\n",
              "      <td>-1.307308</td>\n",
              "      <td>0.519499</td>\n",
              "      <td>-0.647021</td>\n",
              "      <td>0.229126</td>\n",
              "      <td>0.537920</td>\n",
              "      <td>0.096529</td>\n",
              "      <td>0.169272</td>\n",
              "      <td>0.194358</td>\n",
              "      <td>0.874101</td>\n",
              "      <td>-0.068565</td>\n",
              "      <td>-0.514697</td>\n",
              "      <td>-0.346802</td>\n",
              "      <td>0.035213</td>\n",
              "      <td>-0.203666</td>\n",
              "      <td>-0.216070</td>\n",
              "      <td>-0.195695</td>\n",
              "      <td>-0.521439</td>\n",
              "      <td>0.021314</td>\n",
              "      <td>-0.365639</td>\n",
              "      <td>0.479228</td>\n",
              "      <td>...</td>\n",
              "      <td>0.499690</td>\n",
              "      <td>0.544143</td>\n",
              "      <td>-0.180028</td>\n",
              "      <td>0.180431</td>\n",
              "      <td>-0.009263</td>\n",
              "      <td>-0.745323</td>\n",
              "      <td>-0.227080</td>\n",
              "      <td>0.292408</td>\n",
              "      <td>-0.240492</td>\n",
              "      <td>0.239435</td>\n",
              "      <td>-0.225935</td>\n",
              "      <td>0.376984</td>\n",
              "      <td>-0.172546</td>\n",
              "      <td>-0.811954</td>\n",
              "      <td>-0.495904</td>\n",
              "      <td>-0.084609</td>\n",
              "      <td>-0.433649</td>\n",
              "      <td>0.375203</td>\n",
              "      <td>-0.835952</td>\n",
              "      <td>-0.211374</td>\n",
              "      <td>0.526285</td>\n",
              "      <td>0.781766</td>\n",
              "      <td>0.236068</td>\n",
              "      <td>-0.035206</td>\n",
              "      <td>-0.580591</td>\n",
              "      <td>0.462592</td>\n",
              "      <td>0.597586</td>\n",
              "      <td>0.184451</td>\n",
              "      <td>0.802938</td>\n",
              "      <td>-0.162514</td>\n",
              "      <td>-0.297305</td>\n",
              "      <td>0.564505</td>\n",
              "      <td>1.189805</td>\n",
              "      <td>0.779893</td>\n",
              "      <td>0.646247</td>\n",
              "      <td>-0.151325</td>\n",
              "      <td>0.311772</td>\n",
              "      <td>-0.350208</td>\n",
              "      <td>0.281016</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1101850</th>\n",
              "      <td>0.174118</td>\n",
              "      <td>-0.418902</td>\n",
              "      <td>0.126907</td>\n",
              "      <td>-0.434909</td>\n",
              "      <td>0.057360</td>\n",
              "      <td>-0.896717</td>\n",
              "      <td>0.104900</td>\n",
              "      <td>-0.071798</td>\n",
              "      <td>0.058149</td>\n",
              "      <td>-0.162548</td>\n",
              "      <td>-0.446979</td>\n",
              "      <td>0.057747</td>\n",
              "      <td>0.534189</td>\n",
              "      <td>-0.334704</td>\n",
              "      <td>-0.095730</td>\n",
              "      <td>-0.196843</td>\n",
              "      <td>1.432983</td>\n",
              "      <td>0.573235</td>\n",
              "      <td>0.092543</td>\n",
              "      <td>-0.137559</td>\n",
              "      <td>-1.550742</td>\n",
              "      <td>-0.268918</td>\n",
              "      <td>-0.264333</td>\n",
              "      <td>0.146961</td>\n",
              "      <td>-0.152437</td>\n",
              "      <td>0.257579</td>\n",
              "      <td>0.177396</td>\n",
              "      <td>0.009939</td>\n",
              "      <td>0.566907</td>\n",
              "      <td>0.520262</td>\n",
              "      <td>-0.442662</td>\n",
              "      <td>-0.034450</td>\n",
              "      <td>-1.323437</td>\n",
              "      <td>0.642263</td>\n",
              "      <td>-0.510872</td>\n",
              "      <td>-0.719326</td>\n",
              "      <td>0.521323</td>\n",
              "      <td>0.450722</td>\n",
              "      <td>-0.078295</td>\n",
              "      <td>0.110865</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.238680</td>\n",
              "      <td>0.787703</td>\n",
              "      <td>-0.319154</td>\n",
              "      <td>-0.201167</td>\n",
              "      <td>-0.531976</td>\n",
              "      <td>-0.758207</td>\n",
              "      <td>-0.194286</td>\n",
              "      <td>-0.754079</td>\n",
              "      <td>0.723428</td>\n",
              "      <td>0.309464</td>\n",
              "      <td>-0.503141</td>\n",
              "      <td>0.033638</td>\n",
              "      <td>0.057217</td>\n",
              "      <td>-1.088598</td>\n",
              "      <td>0.526985</td>\n",
              "      <td>0.141639</td>\n",
              "      <td>0.412071</td>\n",
              "      <td>0.126281</td>\n",
              "      <td>-0.125068</td>\n",
              "      <td>-0.459249</td>\n",
              "      <td>0.299862</td>\n",
              "      <td>-0.397002</td>\n",
              "      <td>-0.182931</td>\n",
              "      <td>-0.719578</td>\n",
              "      <td>-0.395005</td>\n",
              "      <td>-0.365402</td>\n",
              "      <td>0.480954</td>\n",
              "      <td>-0.213679</td>\n",
              "      <td>0.428882</td>\n",
              "      <td>-0.119099</td>\n",
              "      <td>0.420797</td>\n",
              "      <td>0.039515</td>\n",
              "      <td>0.934612</td>\n",
              "      <td>-0.189791</td>\n",
              "      <td>0.797531</td>\n",
              "      <td>0.288263</td>\n",
              "      <td>0.212681</td>\n",
              "      <td>0.381630</td>\n",
              "      <td>-0.334163</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1336878</th>\n",
              "      <td>-0.457677</td>\n",
              "      <td>-0.075856</td>\n",
              "      <td>-0.706770</td>\n",
              "      <td>0.232660</td>\n",
              "      <td>-0.142577</td>\n",
              "      <td>0.157583</td>\n",
              "      <td>-0.518147</td>\n",
              "      <td>0.305940</td>\n",
              "      <td>-0.658546</td>\n",
              "      <td>0.092074</td>\n",
              "      <td>-0.138651</td>\n",
              "      <td>-0.315813</td>\n",
              "      <td>0.693731</td>\n",
              "      <td>-0.100878</td>\n",
              "      <td>-0.459728</td>\n",
              "      <td>0.764480</td>\n",
              "      <td>0.230007</td>\n",
              "      <td>0.411774</td>\n",
              "      <td>-0.775120</td>\n",
              "      <td>0.495443</td>\n",
              "      <td>0.641672</td>\n",
              "      <td>0.166622</td>\n",
              "      <td>1.113050</td>\n",
              "      <td>-0.514069</td>\n",
              "      <td>-0.108101</td>\n",
              "      <td>0.563612</td>\n",
              "      <td>0.163444</td>\n",
              "      <td>1.052690</td>\n",
              "      <td>-0.263543</td>\n",
              "      <td>0.617824</td>\n",
              "      <td>-0.512447</td>\n",
              "      <td>-0.632055</td>\n",
              "      <td>-1.057613</td>\n",
              "      <td>0.164391</td>\n",
              "      <td>-0.715569</td>\n",
              "      <td>-0.747823</td>\n",
              "      <td>0.537585</td>\n",
              "      <td>-0.116105</td>\n",
              "      <td>-0.205757</td>\n",
              "      <td>-0.311762</td>\n",
              "      <td>...</td>\n",
              "      <td>0.320401</td>\n",
              "      <td>0.522424</td>\n",
              "      <td>0.200598</td>\n",
              "      <td>-0.037448</td>\n",
              "      <td>-1.278617</td>\n",
              "      <td>0.799516</td>\n",
              "      <td>0.575198</td>\n",
              "      <td>-0.877052</td>\n",
              "      <td>0.635505</td>\n",
              "      <td>0.795541</td>\n",
              "      <td>-0.157591</td>\n",
              "      <td>-1.067630</td>\n",
              "      <td>-0.596872</td>\n",
              "      <td>-1.084220</td>\n",
              "      <td>-0.044821</td>\n",
              "      <td>0.102162</td>\n",
              "      <td>-0.929092</td>\n",
              "      <td>0.465053</td>\n",
              "      <td>-1.074324</td>\n",
              "      <td>-0.832807</td>\n",
              "      <td>0.240991</td>\n",
              "      <td>0.076727</td>\n",
              "      <td>0.734057</td>\n",
              "      <td>-0.662209</td>\n",
              "      <td>-1.222314</td>\n",
              "      <td>-0.415042</td>\n",
              "      <td>0.937100</td>\n",
              "      <td>-0.198996</td>\n",
              "      <td>0.577093</td>\n",
              "      <td>-0.224453</td>\n",
              "      <td>0.332408</td>\n",
              "      <td>0.403299</td>\n",
              "      <td>0.177534</td>\n",
              "      <td>0.442314</td>\n",
              "      <td>0.617622</td>\n",
              "      <td>-0.365555</td>\n",
              "      <td>1.018285</td>\n",
              "      <td>-0.881188</td>\n",
              "      <td>0.270753</td>\n",
              "      <td>9.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1515524</th>\n",
              "      <td>0.007245</td>\n",
              "      <td>0.082101</td>\n",
              "      <td>0.480947</td>\n",
              "      <td>-0.876973</td>\n",
              "      <td>0.781694</td>\n",
              "      <td>0.141359</td>\n",
              "      <td>0.211259</td>\n",
              "      <td>0.517827</td>\n",
              "      <td>0.427663</td>\n",
              "      <td>-0.276976</td>\n",
              "      <td>-0.729186</td>\n",
              "      <td>0.689607</td>\n",
              "      <td>-0.346114</td>\n",
              "      <td>-0.488435</td>\n",
              "      <td>-0.107165</td>\n",
              "      <td>0.679893</td>\n",
              "      <td>-0.174342</td>\n",
              "      <td>-0.128430</td>\n",
              "      <td>-0.767647</td>\n",
              "      <td>-0.123611</td>\n",
              "      <td>-0.598480</td>\n",
              "      <td>0.562878</td>\n",
              "      <td>0.065125</td>\n",
              "      <td>-0.036792</td>\n",
              "      <td>-0.444272</td>\n",
              "      <td>0.644067</td>\n",
              "      <td>-0.392876</td>\n",
              "      <td>-0.186923</td>\n",
              "      <td>0.489799</td>\n",
              "      <td>-0.208338</td>\n",
              "      <td>-0.433074</td>\n",
              "      <td>0.113006</td>\n",
              "      <td>-0.877710</td>\n",
              "      <td>0.615984</td>\n",
              "      <td>-0.539929</td>\n",
              "      <td>-0.419203</td>\n",
              "      <td>-0.064564</td>\n",
              "      <td>0.162514</td>\n",
              "      <td>0.431589</td>\n",
              "      <td>0.370070</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.064631</td>\n",
              "      <td>0.465737</td>\n",
              "      <td>-0.277834</td>\n",
              "      <td>-0.039925</td>\n",
              "      <td>-0.605255</td>\n",
              "      <td>0.663218</td>\n",
              "      <td>0.549541</td>\n",
              "      <td>0.555771</td>\n",
              "      <td>-0.323215</td>\n",
              "      <td>0.490686</td>\n",
              "      <td>-0.107345</td>\n",
              "      <td>0.499525</td>\n",
              "      <td>0.028085</td>\n",
              "      <td>-0.508488</td>\n",
              "      <td>-0.249464</td>\n",
              "      <td>-0.306733</td>\n",
              "      <td>-0.254856</td>\n",
              "      <td>-0.032120</td>\n",
              "      <td>0.156723</td>\n",
              "      <td>-0.049597</td>\n",
              "      <td>0.677438</td>\n",
              "      <td>-0.433834</td>\n",
              "      <td>0.738120</td>\n",
              "      <td>-0.566311</td>\n",
              "      <td>-0.149444</td>\n",
              "      <td>0.222702</td>\n",
              "      <td>0.109430</td>\n",
              "      <td>-0.059463</td>\n",
              "      <td>0.618458</td>\n",
              "      <td>-0.496747</td>\n",
              "      <td>-0.107600</td>\n",
              "      <td>0.628540</td>\n",
              "      <td>-0.421982</td>\n",
              "      <td>0.591190</td>\n",
              "      <td>-0.241068</td>\n",
              "      <td>0.308011</td>\n",
              "      <td>0.698082</td>\n",
              "      <td>-0.528992</td>\n",
              "      <td>-0.688370</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1606427</th>\n",
              "      <td>-0.975989</td>\n",
              "      <td>0.187017</td>\n",
              "      <td>-0.988799</td>\n",
              "      <td>1.767161</td>\n",
              "      <td>0.769870</td>\n",
              "      <td>-3.763815</td>\n",
              "      <td>0.132515</td>\n",
              "      <td>-2.558197</td>\n",
              "      <td>-0.826874</td>\n",
              "      <td>2.000220</td>\n",
              "      <td>-2.660278</td>\n",
              "      <td>1.376043</td>\n",
              "      <td>0.947726</td>\n",
              "      <td>0.849836</td>\n",
              "      <td>-0.564574</td>\n",
              "      <td>0.569658</td>\n",
              "      <td>1.704702</td>\n",
              "      <td>0.489982</td>\n",
              "      <td>0.451404</td>\n",
              "      <td>0.113303</td>\n",
              "      <td>-1.191751</td>\n",
              "      <td>1.018100</td>\n",
              "      <td>0.218787</td>\n",
              "      <td>-0.062853</td>\n",
              "      <td>1.258830</td>\n",
              "      <td>0.398644</td>\n",
              "      <td>-2.006275</td>\n",
              "      <td>-0.128674</td>\n",
              "      <td>0.522428</td>\n",
              "      <td>1.066818</td>\n",
              "      <td>-1.485506</td>\n",
              "      <td>0.981592</td>\n",
              "      <td>1.776454</td>\n",
              "      <td>-1.917482</td>\n",
              "      <td>0.974292</td>\n",
              "      <td>2.185344</td>\n",
              "      <td>-0.713491</td>\n",
              "      <td>0.710392</td>\n",
              "      <td>0.642025</td>\n",
              "      <td>-1.820679</td>\n",
              "      <td>...</td>\n",
              "      <td>1.335238</td>\n",
              "      <td>-0.702568</td>\n",
              "      <td>0.713758</td>\n",
              "      <td>1.045334</td>\n",
              "      <td>-1.109869</td>\n",
              "      <td>-3.278014</td>\n",
              "      <td>-0.599495</td>\n",
              "      <td>1.143047</td>\n",
              "      <td>0.110142</td>\n",
              "      <td>-0.009669</td>\n",
              "      <td>-0.992619</td>\n",
              "      <td>0.672795</td>\n",
              "      <td>-0.524433</td>\n",
              "      <td>-2.190053</td>\n",
              "      <td>-0.794319</td>\n",
              "      <td>0.600441</td>\n",
              "      <td>0.505897</td>\n",
              "      <td>-1.153681</td>\n",
              "      <td>-0.208658</td>\n",
              "      <td>1.793183</td>\n",
              "      <td>0.481754</td>\n",
              "      <td>-0.843747</td>\n",
              "      <td>-1.009674</td>\n",
              "      <td>-0.430130</td>\n",
              "      <td>-1.588800</td>\n",
              "      <td>-0.952801</td>\n",
              "      <td>-1.447628</td>\n",
              "      <td>-0.290529</td>\n",
              "      <td>1.927022</td>\n",
              "      <td>1.435307</td>\n",
              "      <td>-1.300712</td>\n",
              "      <td>0.099208</td>\n",
              "      <td>0.110130</td>\n",
              "      <td>0.789594</td>\n",
              "      <td>-0.313303</td>\n",
              "      <td>-0.732405</td>\n",
              "      <td>-2.143843</td>\n",
              "      <td>2.168377</td>\n",
              "      <td>-2.042220</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 1025 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             avg 0     avg 1     avg 2  ...   max 254   max 255  n_papers\n",
              "authorID                                ...                              \n",
              "1036332  -0.670915 -0.359066  0.032644  ... -0.350208  0.281016      10.0\n",
              "1101850   0.174118 -0.418902  0.126907  ...  0.381630 -0.334163      10.0\n",
              "1336878  -0.457677 -0.075856 -0.706770  ... -0.881188  0.270753       9.0\n",
              "1515524   0.007245  0.082101  0.480947  ... -0.528992 -0.688370      10.0\n",
              "1606427  -0.975989  0.187017 -0.988799  ...  2.168377 -2.042220       1.0\n",
              "\n",
              "[5 rows x 1025 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "id": "6YQAAe5i0gfh",
        "outputId": "9e31c1c0-a084-4c5a-c16e-d7f351ebbc9b"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pc = PCA(64)\n",
        "non_avg = author_embeds.iloc[:,256:]\n",
        "authors_mini = pc.fit_transform(non_avg)\n",
        "authors_mini = pd.DataFrame(authors_mini,columns=[\"auth pca\"+str(i) for i in range(64)],index=non_avg.index)\n",
        "authors_mini.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>auth pca0</th>\n",
              "      <th>auth pca1</th>\n",
              "      <th>auth pca2</th>\n",
              "      <th>auth pca3</th>\n",
              "      <th>auth pca4</th>\n",
              "      <th>auth pca5</th>\n",
              "      <th>auth pca6</th>\n",
              "      <th>auth pca7</th>\n",
              "      <th>auth pca8</th>\n",
              "      <th>auth pca9</th>\n",
              "      <th>auth pca10</th>\n",
              "      <th>auth pca11</th>\n",
              "      <th>auth pca12</th>\n",
              "      <th>auth pca13</th>\n",
              "      <th>auth pca14</th>\n",
              "      <th>auth pca15</th>\n",
              "      <th>auth pca16</th>\n",
              "      <th>auth pca17</th>\n",
              "      <th>auth pca18</th>\n",
              "      <th>auth pca19</th>\n",
              "      <th>auth pca20</th>\n",
              "      <th>auth pca21</th>\n",
              "      <th>auth pca22</th>\n",
              "      <th>auth pca23</th>\n",
              "      <th>auth pca24</th>\n",
              "      <th>auth pca25</th>\n",
              "      <th>auth pca26</th>\n",
              "      <th>auth pca27</th>\n",
              "      <th>auth pca28</th>\n",
              "      <th>auth pca29</th>\n",
              "      <th>auth pca30</th>\n",
              "      <th>auth pca31</th>\n",
              "      <th>auth pca32</th>\n",
              "      <th>auth pca33</th>\n",
              "      <th>auth pca34</th>\n",
              "      <th>auth pca35</th>\n",
              "      <th>auth pca36</th>\n",
              "      <th>auth pca37</th>\n",
              "      <th>auth pca38</th>\n",
              "      <th>auth pca39</th>\n",
              "      <th>auth pca40</th>\n",
              "      <th>auth pca41</th>\n",
              "      <th>auth pca42</th>\n",
              "      <th>auth pca43</th>\n",
              "      <th>auth pca44</th>\n",
              "      <th>auth pca45</th>\n",
              "      <th>auth pca46</th>\n",
              "      <th>auth pca47</th>\n",
              "      <th>auth pca48</th>\n",
              "      <th>auth pca49</th>\n",
              "      <th>auth pca50</th>\n",
              "      <th>auth pca51</th>\n",
              "      <th>auth pca52</th>\n",
              "      <th>auth pca53</th>\n",
              "      <th>auth pca54</th>\n",
              "      <th>auth pca55</th>\n",
              "      <th>auth pca56</th>\n",
              "      <th>auth pca57</th>\n",
              "      <th>auth pca58</th>\n",
              "      <th>auth pca59</th>\n",
              "      <th>auth pca60</th>\n",
              "      <th>auth pca61</th>\n",
              "      <th>auth pca62</th>\n",
              "      <th>auth pca63</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>authorID</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1036332</th>\n",
              "      <td>-1.888684</td>\n",
              "      <td>2.597460</td>\n",
              "      <td>-3.433860</td>\n",
              "      <td>-3.167465</td>\n",
              "      <td>0.334390</td>\n",
              "      <td>-0.784500</td>\n",
              "      <td>-0.515614</td>\n",
              "      <td>-0.659675</td>\n",
              "      <td>-0.064631</td>\n",
              "      <td>0.770067</td>\n",
              "      <td>-0.073511</td>\n",
              "      <td>-0.476651</td>\n",
              "      <td>-0.673218</td>\n",
              "      <td>-0.238873</td>\n",
              "      <td>-0.136046</td>\n",
              "      <td>-0.312369</td>\n",
              "      <td>0.279904</td>\n",
              "      <td>0.229094</td>\n",
              "      <td>0.266010</td>\n",
              "      <td>0.755295</td>\n",
              "      <td>-0.043433</td>\n",
              "      <td>0.605252</td>\n",
              "      <td>-1.349813</td>\n",
              "      <td>-0.135789</td>\n",
              "      <td>0.088865</td>\n",
              "      <td>0.093422</td>\n",
              "      <td>0.600559</td>\n",
              "      <td>0.874288</td>\n",
              "      <td>-0.787676</td>\n",
              "      <td>-0.261342</td>\n",
              "      <td>-0.201010</td>\n",
              "      <td>-0.870125</td>\n",
              "      <td>0.429543</td>\n",
              "      <td>-0.819759</td>\n",
              "      <td>-0.788371</td>\n",
              "      <td>-0.159981</td>\n",
              "      <td>-0.379147</td>\n",
              "      <td>0.872277</td>\n",
              "      <td>-0.794453</td>\n",
              "      <td>-0.798592</td>\n",
              "      <td>0.365841</td>\n",
              "      <td>-0.099546</td>\n",
              "      <td>-0.109948</td>\n",
              "      <td>-0.472190</td>\n",
              "      <td>0.462936</td>\n",
              "      <td>0.626468</td>\n",
              "      <td>-0.652985</td>\n",
              "      <td>-1.306869</td>\n",
              "      <td>0.294770</td>\n",
              "      <td>-0.184049</td>\n",
              "      <td>1.241711</td>\n",
              "      <td>0.432382</td>\n",
              "      <td>0.283727</td>\n",
              "      <td>-0.798514</td>\n",
              "      <td>0.269531</td>\n",
              "      <td>-0.405486</td>\n",
              "      <td>0.540251</td>\n",
              "      <td>0.680155</td>\n",
              "      <td>0.560018</td>\n",
              "      <td>0.139965</td>\n",
              "      <td>0.094844</td>\n",
              "      <td>-0.067875</td>\n",
              "      <td>0.229927</td>\n",
              "      <td>0.596025</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1101850</th>\n",
              "      <td>-4.892186</td>\n",
              "      <td>-0.927624</td>\n",
              "      <td>3.559977</td>\n",
              "      <td>-2.071169</td>\n",
              "      <td>0.133569</td>\n",
              "      <td>-0.121129</td>\n",
              "      <td>2.099660</td>\n",
              "      <td>-1.282479</td>\n",
              "      <td>-0.141757</td>\n",
              "      <td>2.376634</td>\n",
              "      <td>0.743500</td>\n",
              "      <td>-1.366278</td>\n",
              "      <td>1.348864</td>\n",
              "      <td>-0.089638</td>\n",
              "      <td>-0.202142</td>\n",
              "      <td>-0.325724</td>\n",
              "      <td>0.697288</td>\n",
              "      <td>-0.302367</td>\n",
              "      <td>-1.063861</td>\n",
              "      <td>1.047689</td>\n",
              "      <td>0.151109</td>\n",
              "      <td>0.058921</td>\n",
              "      <td>-0.525410</td>\n",
              "      <td>-1.710185</td>\n",
              "      <td>-0.669846</td>\n",
              "      <td>1.675405</td>\n",
              "      <td>-1.003668</td>\n",
              "      <td>-0.913637</td>\n",
              "      <td>1.068482</td>\n",
              "      <td>-0.785899</td>\n",
              "      <td>-1.255753</td>\n",
              "      <td>0.085179</td>\n",
              "      <td>-0.238045</td>\n",
              "      <td>1.326020</td>\n",
              "      <td>0.712644</td>\n",
              "      <td>-0.596906</td>\n",
              "      <td>0.459797</td>\n",
              "      <td>-0.306317</td>\n",
              "      <td>-0.345320</td>\n",
              "      <td>0.459910</td>\n",
              "      <td>0.394832</td>\n",
              "      <td>1.242853</td>\n",
              "      <td>-1.335007</td>\n",
              "      <td>-0.500383</td>\n",
              "      <td>0.615085</td>\n",
              "      <td>-0.342108</td>\n",
              "      <td>0.623923</td>\n",
              "      <td>-0.383646</td>\n",
              "      <td>1.060320</td>\n",
              "      <td>-0.485733</td>\n",
              "      <td>-1.095889</td>\n",
              "      <td>0.645656</td>\n",
              "      <td>-0.529546</td>\n",
              "      <td>-0.320823</td>\n",
              "      <td>0.582986</td>\n",
              "      <td>-0.265075</td>\n",
              "      <td>0.522767</td>\n",
              "      <td>0.139836</td>\n",
              "      <td>0.942211</td>\n",
              "      <td>0.500402</td>\n",
              "      <td>-0.101058</td>\n",
              "      <td>-0.061338</td>\n",
              "      <td>-1.019036</td>\n",
              "      <td>0.787626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1336878</th>\n",
              "      <td>-3.996605</td>\n",
              "      <td>0.912660</td>\n",
              "      <td>3.227314</td>\n",
              "      <td>-0.415848</td>\n",
              "      <td>2.946198</td>\n",
              "      <td>1.063094</td>\n",
              "      <td>-0.565784</td>\n",
              "      <td>-1.200454</td>\n",
              "      <td>0.196007</td>\n",
              "      <td>0.540545</td>\n",
              "      <td>0.619977</td>\n",
              "      <td>-1.311934</td>\n",
              "      <td>1.694404</td>\n",
              "      <td>-0.828285</td>\n",
              "      <td>-1.785683</td>\n",
              "      <td>-0.512160</td>\n",
              "      <td>-0.927765</td>\n",
              "      <td>-1.934803</td>\n",
              "      <td>-0.959286</td>\n",
              "      <td>0.484097</td>\n",
              "      <td>-1.236353</td>\n",
              "      <td>0.154546</td>\n",
              "      <td>-0.263917</td>\n",
              "      <td>-1.160840</td>\n",
              "      <td>-0.819463</td>\n",
              "      <td>0.523413</td>\n",
              "      <td>-0.100531</td>\n",
              "      <td>0.062835</td>\n",
              "      <td>0.494515</td>\n",
              "      <td>0.722098</td>\n",
              "      <td>0.495595</td>\n",
              "      <td>1.582544</td>\n",
              "      <td>-0.098640</td>\n",
              "      <td>-0.661285</td>\n",
              "      <td>-0.308787</td>\n",
              "      <td>0.586518</td>\n",
              "      <td>-0.128868</td>\n",
              "      <td>0.556844</td>\n",
              "      <td>0.620096</td>\n",
              "      <td>-0.277864</td>\n",
              "      <td>0.431061</td>\n",
              "      <td>-1.698302</td>\n",
              "      <td>0.883061</td>\n",
              "      <td>-0.177758</td>\n",
              "      <td>1.594602</td>\n",
              "      <td>-1.163703</td>\n",
              "      <td>0.593336</td>\n",
              "      <td>-0.117988</td>\n",
              "      <td>0.729866</td>\n",
              "      <td>-2.000705</td>\n",
              "      <td>0.907523</td>\n",
              "      <td>-0.146293</td>\n",
              "      <td>-0.274439</td>\n",
              "      <td>-0.147894</td>\n",
              "      <td>0.022547</td>\n",
              "      <td>0.448054</td>\n",
              "      <td>-0.527059</td>\n",
              "      <td>-0.101904</td>\n",
              "      <td>0.430370</td>\n",
              "      <td>0.536216</td>\n",
              "      <td>-0.429230</td>\n",
              "      <td>1.200396</td>\n",
              "      <td>-0.224363</td>\n",
              "      <td>1.226513</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1515524</th>\n",
              "      <td>-3.888946</td>\n",
              "      <td>-0.177399</td>\n",
              "      <td>1.045000</td>\n",
              "      <td>-1.251053</td>\n",
              "      <td>2.047506</td>\n",
              "      <td>-0.562977</td>\n",
              "      <td>-1.030665</td>\n",
              "      <td>0.079445</td>\n",
              "      <td>-0.825412</td>\n",
              "      <td>0.965286</td>\n",
              "      <td>-0.686187</td>\n",
              "      <td>0.537380</td>\n",
              "      <td>0.804977</td>\n",
              "      <td>-0.497985</td>\n",
              "      <td>3.058448</td>\n",
              "      <td>0.341666</td>\n",
              "      <td>-0.899289</td>\n",
              "      <td>0.083823</td>\n",
              "      <td>0.190240</td>\n",
              "      <td>-0.537165</td>\n",
              "      <td>0.196638</td>\n",
              "      <td>-1.437837</td>\n",
              "      <td>0.232475</td>\n",
              "      <td>0.318696</td>\n",
              "      <td>-0.165605</td>\n",
              "      <td>1.341709</td>\n",
              "      <td>-0.713093</td>\n",
              "      <td>-0.102414</td>\n",
              "      <td>1.417433</td>\n",
              "      <td>0.575318</td>\n",
              "      <td>0.126305</td>\n",
              "      <td>-0.565235</td>\n",
              "      <td>0.081869</td>\n",
              "      <td>0.223006</td>\n",
              "      <td>0.578729</td>\n",
              "      <td>-0.516715</td>\n",
              "      <td>-0.546091</td>\n",
              "      <td>-0.346460</td>\n",
              "      <td>0.388159</td>\n",
              "      <td>0.687787</td>\n",
              "      <td>0.241704</td>\n",
              "      <td>0.051588</td>\n",
              "      <td>0.548510</td>\n",
              "      <td>-0.592342</td>\n",
              "      <td>-0.344168</td>\n",
              "      <td>-0.136954</td>\n",
              "      <td>0.456785</td>\n",
              "      <td>0.338268</td>\n",
              "      <td>1.014596</td>\n",
              "      <td>0.540589</td>\n",
              "      <td>0.263777</td>\n",
              "      <td>0.080614</td>\n",
              "      <td>-0.320926</td>\n",
              "      <td>0.112856</td>\n",
              "      <td>0.024145</td>\n",
              "      <td>0.265165</td>\n",
              "      <td>-0.298112</td>\n",
              "      <td>0.348617</td>\n",
              "      <td>-0.167736</td>\n",
              "      <td>-0.116046</td>\n",
              "      <td>-0.023835</td>\n",
              "      <td>-0.093525</td>\n",
              "      <td>-0.273720</td>\n",
              "      <td>-0.127282</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1606427</th>\n",
              "      <td>14.961643</td>\n",
              "      <td>1.738277</td>\n",
              "      <td>1.988560</td>\n",
              "      <td>-6.834096</td>\n",
              "      <td>-4.206595</td>\n",
              "      <td>2.139047</td>\n",
              "      <td>-2.518138</td>\n",
              "      <td>-1.278923</td>\n",
              "      <td>5.096857</td>\n",
              "      <td>0.508537</td>\n",
              "      <td>2.123072</td>\n",
              "      <td>-0.195582</td>\n",
              "      <td>-5.474321</td>\n",
              "      <td>2.057532</td>\n",
              "      <td>-2.179230</td>\n",
              "      <td>-0.778114</td>\n",
              "      <td>0.198619</td>\n",
              "      <td>2.291740</td>\n",
              "      <td>2.148350</td>\n",
              "      <td>1.546222</td>\n",
              "      <td>0.747195</td>\n",
              "      <td>-0.585824</td>\n",
              "      <td>0.258602</td>\n",
              "      <td>-0.687538</td>\n",
              "      <td>-1.800539</td>\n",
              "      <td>2.211969</td>\n",
              "      <td>-0.338183</td>\n",
              "      <td>1.778699</td>\n",
              "      <td>3.316535</td>\n",
              "      <td>-3.104122</td>\n",
              "      <td>0.178432</td>\n",
              "      <td>0.261691</td>\n",
              "      <td>-2.253872</td>\n",
              "      <td>3.499701</td>\n",
              "      <td>3.706689</td>\n",
              "      <td>1.592257</td>\n",
              "      <td>0.271211</td>\n",
              "      <td>-2.187355</td>\n",
              "      <td>-2.559459</td>\n",
              "      <td>1.255149</td>\n",
              "      <td>-0.973509</td>\n",
              "      <td>-1.492862</td>\n",
              "      <td>-0.230083</td>\n",
              "      <td>-0.432149</td>\n",
              "      <td>-2.007672</td>\n",
              "      <td>-0.636527</td>\n",
              "      <td>0.248641</td>\n",
              "      <td>-0.753043</td>\n",
              "      <td>-2.966186</td>\n",
              "      <td>-2.406484</td>\n",
              "      <td>-2.307063</td>\n",
              "      <td>-0.253537</td>\n",
              "      <td>0.055402</td>\n",
              "      <td>-0.240459</td>\n",
              "      <td>1.826808</td>\n",
              "      <td>-0.104640</td>\n",
              "      <td>-1.834434</td>\n",
              "      <td>-0.388971</td>\n",
              "      <td>0.980944</td>\n",
              "      <td>0.074474</td>\n",
              "      <td>3.287161</td>\n",
              "      <td>-0.193834</td>\n",
              "      <td>-0.466919</td>\n",
              "      <td>-0.373621</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          auth pca0  auth pca1  auth pca2  ...  auth pca61  auth pca62  auth pca63\n",
              "authorID                                   ...                                    \n",
              "1036332   -1.888684   2.597460  -3.433860  ...   -0.067875    0.229927    0.596025\n",
              "1101850   -4.892186  -0.927624   3.559977  ...   -0.061338   -1.019036    0.787626\n",
              "1336878   -3.996605   0.912660   3.227314  ...    1.200396   -0.224363    1.226513\n",
              "1515524   -3.888946  -0.177399   1.045000  ...   -0.093525   -0.273720   -0.127282\n",
              "1606427   14.961643   1.738277   1.988560  ...   -0.193834   -0.466919   -0.373621\n",
              "\n",
              "[5 rows x 64 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTSI48hzLDlv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "outputId": "62601f92-7b01-403d-f58d-93054d541a47"
      },
      "source": [
        "node_cl = pd.read_csv(\"node_cluster_data.csv\")\n",
        "### DROP Categorical cluster node variable it's useless\n",
        "to_drop = ['node_cluster_'+str(i) for i in range(1,75)] + ['author_node_number']\n",
        "node_cl.drop(columns=to_drop,inplace=True)\n",
        "#node_cl = node_cl[[\"authorID\",\"degree\",\"core_n\",\"avg_nghbr_degree\"]]\n",
        "node_cl.set_index(\"authorID\",inplace=True)\n",
        "node_cl.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>c_mean</th>\n",
              "      <th>c_var</th>\n",
              "      <th>c_quantile_0%</th>\n",
              "      <th>c_quantile_10%</th>\n",
              "      <th>c_quantile_20%</th>\n",
              "      <th>c_quantile_30%</th>\n",
              "      <th>c_quantile_40%</th>\n",
              "      <th>c_quantile_50%</th>\n",
              "      <th>c_quantile_60%</th>\n",
              "      <th>c_quantile_70%</th>\n",
              "      <th>c_quantile_80%</th>\n",
              "      <th>c_quantile_90%</th>\n",
              "      <th>c_quantile_100%</th>\n",
              "      <th>degree</th>\n",
              "      <th>core_n</th>\n",
              "      <th>avg_nghbr_degree</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>authorID</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2002218453</th>\n",
              "      <td>10.625000</td>\n",
              "      <td>12.993817</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.7</td>\n",
              "      <td>3.4</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>5.2</td>\n",
              "      <td>9.4</td>\n",
              "      <td>14.8</td>\n",
              "      <td>24.6</td>\n",
              "      <td>40.0</td>\n",
              "      <td>19</td>\n",
              "      <td>11</td>\n",
              "      <td>12.368421</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999212242</th>\n",
              "      <td>7.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>10.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2032640503</th>\n",
              "      <td>6.826087</td>\n",
              "      <td>6.393549</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>7.4</td>\n",
              "      <td>10.8</td>\n",
              "      <td>16.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>12</td>\n",
              "      <td>8</td>\n",
              "      <td>8.416667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2475931411</th>\n",
              "      <td>16.000000</td>\n",
              "      <td>31.892677</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>4.8</td>\n",
              "      <td>11.1</td>\n",
              "      <td>12.6</td>\n",
              "      <td>37.3</td>\n",
              "      <td>94.0</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>10.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2477743428</th>\n",
              "      <td>26.714285</td>\n",
              "      <td>44.481136</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.6</td>\n",
              "      <td>2.2</td>\n",
              "      <td>2.8</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>9.6</td>\n",
              "      <td>19.6</td>\n",
              "      <td>36.4</td>\n",
              "      <td>74.0</td>\n",
              "      <td>122.0</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>10.250000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               c_mean      c_var  ...  core_n  avg_nghbr_degree\n",
              "authorID                          ...                          \n",
              "2002218453  10.625000  12.993817  ...      11         12.368421\n",
              "1999212242   7.000000   5.000000  ...       8         10.250000\n",
              "2032640503   6.826087   6.393549  ...       8          8.416667\n",
              "2475931411  16.000000  31.892677  ...       8         10.250000\n",
              "2477743428  26.714285  44.481136  ...       8         10.250000\n",
              "\n",
              "[5 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGqRKTekJe0H"
      },
      "source": [
        "dw_emb = pd.read_csv('colab_net_deepwalk_embeddings.txt', sep=\" \" , skiprows=1, header=None)\n",
        "dw_emb.columns = ['authorID' if i==0 else f'dw_cp{i}' for i in range(dw_emb.shape[1])]\n",
        "#dw_emb.set_index(\"authorID\",inplace=)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GV7dZoZlQyl0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "outputId": "11155912-49f5-4a42-dea6-4111c39e5544"
      },
      "source": [
        "n_papers = pd.DataFrame(author_embeds['n_papers'], columns=['n_papers'], index=author_embeds.index)\n",
        "n_papers.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>n_papers</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>authorID</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1036332</th>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1101850</th>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1336878</th>\n",
              "      <td>9.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1515524</th>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1606427</th>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          n_papers\n",
              "authorID          \n",
              "1036332       10.0\n",
              "1101850       10.0\n",
              "1336878        9.0\n",
              "1515524       10.0\n",
              "1606427        1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "RhrFZvERO5kC",
        "outputId": "44f685b3-46cd-4b45-d904-ad1a1f54f359"
      },
      "source": [
        "kw_emb = pd.read_csv(\"author_keywds_AHMED.csv\")\n",
        "kw_emb.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>authorID</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "      <th>55</th>\n",
              "      <th>56</th>\n",
              "      <th>57</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1036332</td>\n",
              "      <td>-0.001448</td>\n",
              "      <td>0.005915</td>\n",
              "      <td>-0.000509</td>\n",
              "      <td>-0.001444</td>\n",
              "      <td>0.004591</td>\n",
              "      <td>0.005749</td>\n",
              "      <td>-0.004986</td>\n",
              "      <td>-0.003163</td>\n",
              "      <td>-0.005998</td>\n",
              "      <td>-0.006371</td>\n",
              "      <td>0.001179</td>\n",
              "      <td>-0.004602</td>\n",
              "      <td>0.007034</td>\n",
              "      <td>-0.000548</td>\n",
              "      <td>-0.007070</td>\n",
              "      <td>-0.001693</td>\n",
              "      <td>0.000880</td>\n",
              "      <td>0.000797</td>\n",
              "      <td>0.002033</td>\n",
              "      <td>0.004233</td>\n",
              "      <td>-0.004055</td>\n",
              "      <td>-0.001624</td>\n",
              "      <td>-0.001595</td>\n",
              "      <td>0.004509</td>\n",
              "      <td>0.007585</td>\n",
              "      <td>-0.006311</td>\n",
              "      <td>-0.000999</td>\n",
              "      <td>-0.002036</td>\n",
              "      <td>-0.004088</td>\n",
              "      <td>0.002935</td>\n",
              "      <td>0.007257</td>\n",
              "      <td>-0.001903</td>\n",
              "      <td>-0.006215</td>\n",
              "      <td>0.007679</td>\n",
              "      <td>0.000152</td>\n",
              "      <td>-0.000554</td>\n",
              "      <td>-0.000112</td>\n",
              "      <td>0.004722</td>\n",
              "      <td>0.002766</td>\n",
              "      <td>-0.003548</td>\n",
              "      <td>-0.007417</td>\n",
              "      <td>0.006346</td>\n",
              "      <td>0.002415</td>\n",
              "      <td>-0.005626</td>\n",
              "      <td>0.000391</td>\n",
              "      <td>0.000503</td>\n",
              "      <td>0.004809</td>\n",
              "      <td>0.000520</td>\n",
              "      <td>0.002058</td>\n",
              "      <td>-0.005602</td>\n",
              "      <td>0.006533</td>\n",
              "      <td>0.005513</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>0.005013</td>\n",
              "      <td>0.002052</td>\n",
              "      <td>-0.003057</td>\n",
              "      <td>-0.005553</td>\n",
              "      <td>0.006534</td>\n",
              "      <td>-0.006660</td>\n",
              "      <td>0.005388</td>\n",
              "      <td>0.002876</td>\n",
              "      <td>0.006540</td>\n",
              "      <td>0.006745</td>\n",
              "      <td>0.007610</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1101850</td>\n",
              "      <td>-0.004350</td>\n",
              "      <td>-0.005770</td>\n",
              "      <td>-0.004901</td>\n",
              "      <td>-0.003269</td>\n",
              "      <td>0.003187</td>\n",
              "      <td>0.003014</td>\n",
              "      <td>-0.002801</td>\n",
              "      <td>0.003896</td>\n",
              "      <td>-0.001379</td>\n",
              "      <td>0.006799</td>\n",
              "      <td>-0.000906</td>\n",
              "      <td>-0.005504</td>\n",
              "      <td>0.002971</td>\n",
              "      <td>-0.007459</td>\n",
              "      <td>-0.004891</td>\n",
              "      <td>-0.001497</td>\n",
              "      <td>-0.000926</td>\n",
              "      <td>-0.001727</td>\n",
              "      <td>-0.005587</td>\n",
              "      <td>-0.006067</td>\n",
              "      <td>-0.001501</td>\n",
              "      <td>0.007053</td>\n",
              "      <td>-0.005986</td>\n",
              "      <td>-0.001209</td>\n",
              "      <td>0.000466</td>\n",
              "      <td>0.001398</td>\n",
              "      <td>-0.000428</td>\n",
              "      <td>-0.001320</td>\n",
              "      <td>-0.003714</td>\n",
              "      <td>-0.004532</td>\n",
              "      <td>-0.006200</td>\n",
              "      <td>0.000738</td>\n",
              "      <td>0.001223</td>\n",
              "      <td>-0.004606</td>\n",
              "      <td>-0.004458</td>\n",
              "      <td>0.001919</td>\n",
              "      <td>0.002238</td>\n",
              "      <td>-0.006016</td>\n",
              "      <td>0.003875</td>\n",
              "      <td>0.002014</td>\n",
              "      <td>0.004882</td>\n",
              "      <td>0.000064</td>\n",
              "      <td>-0.006600</td>\n",
              "      <td>0.002346</td>\n",
              "      <td>0.005872</td>\n",
              "      <td>0.007262</td>\n",
              "      <td>-0.003594</td>\n",
              "      <td>-0.000818</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.003670</td>\n",
              "      <td>-0.003555</td>\n",
              "      <td>0.007660</td>\n",
              "      <td>0.005929</td>\n",
              "      <td>-0.004833</td>\n",
              "      <td>-0.001249</td>\n",
              "      <td>-0.004434</td>\n",
              "      <td>-0.004301</td>\n",
              "      <td>0.001798</td>\n",
              "      <td>0.002644</td>\n",
              "      <td>-0.000286</td>\n",
              "      <td>-0.001975</td>\n",
              "      <td>0.000350</td>\n",
              "      <td>-0.001531</td>\n",
              "      <td>-0.003356</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1336878</td>\n",
              "      <td>0.007272</td>\n",
              "      <td>0.001268</td>\n",
              "      <td>-0.004422</td>\n",
              "      <td>-0.005841</td>\n",
              "      <td>-0.002690</td>\n",
              "      <td>-0.000401</td>\n",
              "      <td>0.001919</td>\n",
              "      <td>-0.004113</td>\n",
              "      <td>0.001136</td>\n",
              "      <td>0.007156</td>\n",
              "      <td>0.000250</td>\n",
              "      <td>-0.004210</td>\n",
              "      <td>-0.001063</td>\n",
              "      <td>0.006351</td>\n",
              "      <td>0.007161</td>\n",
              "      <td>-0.006540</td>\n",
              "      <td>0.007320</td>\n",
              "      <td>-0.006076</td>\n",
              "      <td>0.007695</td>\n",
              "      <td>-0.003782</td>\n",
              "      <td>-0.001744</td>\n",
              "      <td>-0.003909</td>\n",
              "      <td>-0.003992</td>\n",
              "      <td>0.002415</td>\n",
              "      <td>-0.007480</td>\n",
              "      <td>-0.007161</td>\n",
              "      <td>-0.003385</td>\n",
              "      <td>-0.007214</td>\n",
              "      <td>0.006967</td>\n",
              "      <td>-0.004896</td>\n",
              "      <td>-0.001689</td>\n",
              "      <td>0.002751</td>\n",
              "      <td>0.002773</td>\n",
              "      <td>0.003575</td>\n",
              "      <td>-0.002886</td>\n",
              "      <td>0.002113</td>\n",
              "      <td>0.003545</td>\n",
              "      <td>-0.006258</td>\n",
              "      <td>-0.005767</td>\n",
              "      <td>0.006883</td>\n",
              "      <td>0.005591</td>\n",
              "      <td>-0.001869</td>\n",
              "      <td>0.000745</td>\n",
              "      <td>0.002099</td>\n",
              "      <td>-0.000348</td>\n",
              "      <td>0.004043</td>\n",
              "      <td>-0.005000</td>\n",
              "      <td>-0.004704</td>\n",
              "      <td>0.002866</td>\n",
              "      <td>0.001804</td>\n",
              "      <td>0.003345</td>\n",
              "      <td>-0.002383</td>\n",
              "      <td>-0.002857</td>\n",
              "      <td>0.006740</td>\n",
              "      <td>0.002607</td>\n",
              "      <td>-0.007753</td>\n",
              "      <td>-0.007416</td>\n",
              "      <td>0.000145</td>\n",
              "      <td>-0.003446</td>\n",
              "      <td>0.002858</td>\n",
              "      <td>-0.001515</td>\n",
              "      <td>-0.006126</td>\n",
              "      <td>-0.000309</td>\n",
              "      <td>0.003714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1515524</td>\n",
              "      <td>0.005060</td>\n",
              "      <td>-0.003722</td>\n",
              "      <td>0.004283</td>\n",
              "      <td>-0.002750</td>\n",
              "      <td>-0.000137</td>\n",
              "      <td>-0.004208</td>\n",
              "      <td>0.001779</td>\n",
              "      <td>0.005762</td>\n",
              "      <td>0.002427</td>\n",
              "      <td>0.000157</td>\n",
              "      <td>-0.004908</td>\n",
              "      <td>0.003978</td>\n",
              "      <td>0.001616</td>\n",
              "      <td>-0.002841</td>\n",
              "      <td>-0.000249</td>\n",
              "      <td>-0.003878</td>\n",
              "      <td>0.005836</td>\n",
              "      <td>-0.003951</td>\n",
              "      <td>0.007382</td>\n",
              "      <td>0.005743</td>\n",
              "      <td>0.006493</td>\n",
              "      <td>0.003315</td>\n",
              "      <td>-0.000081</td>\n",
              "      <td>0.001771</td>\n",
              "      <td>-0.005255</td>\n",
              "      <td>0.004866</td>\n",
              "      <td>-0.003454</td>\n",
              "      <td>-0.002683</td>\n",
              "      <td>-0.002002</td>\n",
              "      <td>0.004655</td>\n",
              "      <td>-0.007248</td>\n",
              "      <td>0.002210</td>\n",
              "      <td>-0.006482</td>\n",
              "      <td>0.004094</td>\n",
              "      <td>0.002389</td>\n",
              "      <td>0.005621</td>\n",
              "      <td>0.002380</td>\n",
              "      <td>-0.005766</td>\n",
              "      <td>0.001689</td>\n",
              "      <td>-0.000885</td>\n",
              "      <td>-0.001696</td>\n",
              "      <td>0.001604</td>\n",
              "      <td>-0.001521</td>\n",
              "      <td>-0.000896</td>\n",
              "      <td>0.007109</td>\n",
              "      <td>0.002954</td>\n",
              "      <td>-0.001629</td>\n",
              "      <td>-0.005539</td>\n",
              "      <td>-0.004413</td>\n",
              "      <td>-0.006690</td>\n",
              "      <td>-0.005202</td>\n",
              "      <td>-0.004700</td>\n",
              "      <td>0.003195</td>\n",
              "      <td>0.005895</td>\n",
              "      <td>0.002592</td>\n",
              "      <td>0.004534</td>\n",
              "      <td>-0.007363</td>\n",
              "      <td>0.005645</td>\n",
              "      <td>0.006898</td>\n",
              "      <td>0.000620</td>\n",
              "      <td>0.000940</td>\n",
              "      <td>-0.005028</td>\n",
              "      <td>0.006392</td>\n",
              "      <td>0.005439</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1606427</td>\n",
              "      <td>-0.007484</td>\n",
              "      <td>0.000104</td>\n",
              "      <td>0.001028</td>\n",
              "      <td>-0.003142</td>\n",
              "      <td>-0.005667</td>\n",
              "      <td>0.002855</td>\n",
              "      <td>-0.007741</td>\n",
              "      <td>-0.004129</td>\n",
              "      <td>-0.004093</td>\n",
              "      <td>-0.006880</td>\n",
              "      <td>0.001667</td>\n",
              "      <td>-0.001442</td>\n",
              "      <td>0.002583</td>\n",
              "      <td>0.000767</td>\n",
              "      <td>0.002078</td>\n",
              "      <td>0.005262</td>\n",
              "      <td>-0.005288</td>\n",
              "      <td>-0.001745</td>\n",
              "      <td>-0.003504</td>\n",
              "      <td>0.003245</td>\n",
              "      <td>0.007296</td>\n",
              "      <td>0.002527</td>\n",
              "      <td>0.004500</td>\n",
              "      <td>-0.003053</td>\n",
              "      <td>0.006363</td>\n",
              "      <td>-0.006841</td>\n",
              "      <td>-0.002968</td>\n",
              "      <td>0.002493</td>\n",
              "      <td>-0.004403</td>\n",
              "      <td>-0.000058</td>\n",
              "      <td>0.003323</td>\n",
              "      <td>0.005582</td>\n",
              "      <td>-0.006956</td>\n",
              "      <td>0.007174</td>\n",
              "      <td>-0.001114</td>\n",
              "      <td>-0.000182</td>\n",
              "      <td>-0.006604</td>\n",
              "      <td>0.001045</td>\n",
              "      <td>0.004899</td>\n",
              "      <td>0.001815</td>\n",
              "      <td>0.007688</td>\n",
              "      <td>-0.005770</td>\n",
              "      <td>-0.004798</td>\n",
              "      <td>-0.002199</td>\n",
              "      <td>0.000803</td>\n",
              "      <td>0.006463</td>\n",
              "      <td>-0.003974</td>\n",
              "      <td>0.005718</td>\n",
              "      <td>-0.004282</td>\n",
              "      <td>-0.003011</td>\n",
              "      <td>-0.007387</td>\n",
              "      <td>-0.002689</td>\n",
              "      <td>-0.006804</td>\n",
              "      <td>0.003769</td>\n",
              "      <td>-0.006224</td>\n",
              "      <td>-0.006819</td>\n",
              "      <td>-0.003412</td>\n",
              "      <td>0.003950</td>\n",
              "      <td>-0.000678</td>\n",
              "      <td>0.002643</td>\n",
              "      <td>0.007413</td>\n",
              "      <td>-0.005458</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.004185</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   authorID         0         1  ...        61        62        63\n",
              "0   1036332 -0.001448  0.005915  ...  0.006540  0.006745  0.007610\n",
              "1   1101850 -0.004350 -0.005770  ...  0.000350 -0.001531 -0.003356\n",
              "2   1336878  0.007272  0.001268  ... -0.006126 -0.000309  0.003714\n",
              "3   1515524  0.005060 -0.003722  ... -0.005028  0.006392  0.005439\n",
              "4   1606427 -0.007484  0.000104  ... -0.005458  0.000005  0.004185\n",
              "\n",
              "[5 rows x 65 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBO-z3fnNjVQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "outputId": "0ac3a422-3643-4f8e-e49e-93e13e0c141b"
      },
      "source": [
        "df_full = author_embeds.iloc[:,:256]\n",
        "#df_full = df_full.merge(authors_mini,on='authorID')\n",
        "df_full = df_full.merge(n_papers,on='authorID')\n",
        "df_full = df_full.merge(node_cl,on='authorID')\n",
        "df_full = df_full.merge(dw_emb,on='authorID')\n",
        "df_full = df_full.merge(kw_emb,on='authorID')\n",
        "df_full.head()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>authorID</th>\n",
              "      <th>avg 0</th>\n",
              "      <th>avg 1</th>\n",
              "      <th>avg 2</th>\n",
              "      <th>avg 3</th>\n",
              "      <th>avg 4</th>\n",
              "      <th>avg 5</th>\n",
              "      <th>avg 6</th>\n",
              "      <th>avg 7</th>\n",
              "      <th>avg 8</th>\n",
              "      <th>avg 9</th>\n",
              "      <th>avg 10</th>\n",
              "      <th>avg 11</th>\n",
              "      <th>avg 12</th>\n",
              "      <th>avg 13</th>\n",
              "      <th>avg 14</th>\n",
              "      <th>avg 15</th>\n",
              "      <th>avg 16</th>\n",
              "      <th>avg 17</th>\n",
              "      <th>avg 18</th>\n",
              "      <th>avg 19</th>\n",
              "      <th>avg 20</th>\n",
              "      <th>avg 21</th>\n",
              "      <th>avg 22</th>\n",
              "      <th>avg 23</th>\n",
              "      <th>avg 24</th>\n",
              "      <th>avg 25</th>\n",
              "      <th>avg 26</th>\n",
              "      <th>avg 27</th>\n",
              "      <th>avg 28</th>\n",
              "      <th>avg 29</th>\n",
              "      <th>avg 30</th>\n",
              "      <th>avg 31</th>\n",
              "      <th>avg 32</th>\n",
              "      <th>avg 33</th>\n",
              "      <th>avg 34</th>\n",
              "      <th>avg 35</th>\n",
              "      <th>avg 36</th>\n",
              "      <th>avg 37</th>\n",
              "      <th>avg 38</th>\n",
              "      <th>...</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "      <th>55</th>\n",
              "      <th>56</th>\n",
              "      <th>57</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1036332</td>\n",
              "      <td>-0.670915</td>\n",
              "      <td>-0.359066</td>\n",
              "      <td>0.032644</td>\n",
              "      <td>0.001299</td>\n",
              "      <td>0.516241</td>\n",
              "      <td>-0.401442</td>\n",
              "      <td>-0.266719</td>\n",
              "      <td>-0.309738</td>\n",
              "      <td>-0.317130</td>\n",
              "      <td>0.317681</td>\n",
              "      <td>-0.330467</td>\n",
              "      <td>0.612775</td>\n",
              "      <td>-0.021864</td>\n",
              "      <td>-0.729525</td>\n",
              "      <td>-0.032873</td>\n",
              "      <td>0.296621</td>\n",
              "      <td>-0.117286</td>\n",
              "      <td>-0.256655</td>\n",
              "      <td>0.706496</td>\n",
              "      <td>-0.183294</td>\n",
              "      <td>-1.307308</td>\n",
              "      <td>0.519499</td>\n",
              "      <td>-0.647021</td>\n",
              "      <td>0.229126</td>\n",
              "      <td>0.537920</td>\n",
              "      <td>0.096529</td>\n",
              "      <td>0.169272</td>\n",
              "      <td>0.194358</td>\n",
              "      <td>0.874101</td>\n",
              "      <td>-0.068565</td>\n",
              "      <td>-0.514697</td>\n",
              "      <td>-0.346802</td>\n",
              "      <td>0.035213</td>\n",
              "      <td>-0.203666</td>\n",
              "      <td>-0.216070</td>\n",
              "      <td>-0.195695</td>\n",
              "      <td>-0.521439</td>\n",
              "      <td>0.021314</td>\n",
              "      <td>-0.365639</td>\n",
              "      <td>...</td>\n",
              "      <td>0.007585</td>\n",
              "      <td>-0.006311</td>\n",
              "      <td>-0.000999</td>\n",
              "      <td>-0.002036</td>\n",
              "      <td>-0.004088</td>\n",
              "      <td>0.002935</td>\n",
              "      <td>0.007257</td>\n",
              "      <td>-0.001903</td>\n",
              "      <td>-0.006215</td>\n",
              "      <td>0.007679</td>\n",
              "      <td>0.000152</td>\n",
              "      <td>-0.000554</td>\n",
              "      <td>-0.000112</td>\n",
              "      <td>0.004722</td>\n",
              "      <td>0.002766</td>\n",
              "      <td>-0.003548</td>\n",
              "      <td>-0.007417</td>\n",
              "      <td>0.006346</td>\n",
              "      <td>0.002415</td>\n",
              "      <td>-0.005626</td>\n",
              "      <td>0.000391</td>\n",
              "      <td>0.000503</td>\n",
              "      <td>0.004809</td>\n",
              "      <td>0.000520</td>\n",
              "      <td>0.002058</td>\n",
              "      <td>-0.005602</td>\n",
              "      <td>0.006533</td>\n",
              "      <td>0.005513</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>0.005013</td>\n",
              "      <td>0.002052</td>\n",
              "      <td>-0.003057</td>\n",
              "      <td>-0.005553</td>\n",
              "      <td>0.006534</td>\n",
              "      <td>-0.006660</td>\n",
              "      <td>0.005388</td>\n",
              "      <td>0.002876</td>\n",
              "      <td>0.006540</td>\n",
              "      <td>0.006745</td>\n",
              "      <td>0.007610</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1101850</td>\n",
              "      <td>0.174118</td>\n",
              "      <td>-0.418902</td>\n",
              "      <td>0.126907</td>\n",
              "      <td>-0.434909</td>\n",
              "      <td>0.057360</td>\n",
              "      <td>-0.896717</td>\n",
              "      <td>0.104900</td>\n",
              "      <td>-0.071798</td>\n",
              "      <td>0.058149</td>\n",
              "      <td>-0.162548</td>\n",
              "      <td>-0.446979</td>\n",
              "      <td>0.057747</td>\n",
              "      <td>0.534189</td>\n",
              "      <td>-0.334704</td>\n",
              "      <td>-0.095730</td>\n",
              "      <td>-0.196843</td>\n",
              "      <td>1.432983</td>\n",
              "      <td>0.573235</td>\n",
              "      <td>0.092543</td>\n",
              "      <td>-0.137559</td>\n",
              "      <td>-1.550742</td>\n",
              "      <td>-0.268918</td>\n",
              "      <td>-0.264333</td>\n",
              "      <td>0.146961</td>\n",
              "      <td>-0.152437</td>\n",
              "      <td>0.257579</td>\n",
              "      <td>0.177396</td>\n",
              "      <td>0.009939</td>\n",
              "      <td>0.566907</td>\n",
              "      <td>0.520262</td>\n",
              "      <td>-0.442662</td>\n",
              "      <td>-0.034450</td>\n",
              "      <td>-1.323437</td>\n",
              "      <td>0.642263</td>\n",
              "      <td>-0.510872</td>\n",
              "      <td>-0.719326</td>\n",
              "      <td>0.521323</td>\n",
              "      <td>0.450722</td>\n",
              "      <td>-0.078295</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000466</td>\n",
              "      <td>0.001398</td>\n",
              "      <td>-0.000428</td>\n",
              "      <td>-0.001320</td>\n",
              "      <td>-0.003714</td>\n",
              "      <td>-0.004532</td>\n",
              "      <td>-0.006200</td>\n",
              "      <td>0.000738</td>\n",
              "      <td>0.001223</td>\n",
              "      <td>-0.004606</td>\n",
              "      <td>-0.004458</td>\n",
              "      <td>0.001919</td>\n",
              "      <td>0.002238</td>\n",
              "      <td>-0.006016</td>\n",
              "      <td>0.003875</td>\n",
              "      <td>0.002014</td>\n",
              "      <td>0.004882</td>\n",
              "      <td>0.000064</td>\n",
              "      <td>-0.006600</td>\n",
              "      <td>0.002346</td>\n",
              "      <td>0.005872</td>\n",
              "      <td>0.007262</td>\n",
              "      <td>-0.003594</td>\n",
              "      <td>-0.000818</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.003670</td>\n",
              "      <td>-0.003555</td>\n",
              "      <td>0.007660</td>\n",
              "      <td>0.005929</td>\n",
              "      <td>-0.004833</td>\n",
              "      <td>-0.001249</td>\n",
              "      <td>-0.004434</td>\n",
              "      <td>-0.004301</td>\n",
              "      <td>0.001798</td>\n",
              "      <td>0.002644</td>\n",
              "      <td>-0.000286</td>\n",
              "      <td>-0.001975</td>\n",
              "      <td>0.000350</td>\n",
              "      <td>-0.001531</td>\n",
              "      <td>-0.003356</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1336878</td>\n",
              "      <td>-0.457677</td>\n",
              "      <td>-0.075856</td>\n",
              "      <td>-0.706770</td>\n",
              "      <td>0.232660</td>\n",
              "      <td>-0.142577</td>\n",
              "      <td>0.157583</td>\n",
              "      <td>-0.518147</td>\n",
              "      <td>0.305940</td>\n",
              "      <td>-0.658546</td>\n",
              "      <td>0.092074</td>\n",
              "      <td>-0.138651</td>\n",
              "      <td>-0.315813</td>\n",
              "      <td>0.693731</td>\n",
              "      <td>-0.100878</td>\n",
              "      <td>-0.459728</td>\n",
              "      <td>0.764480</td>\n",
              "      <td>0.230007</td>\n",
              "      <td>0.411774</td>\n",
              "      <td>-0.775120</td>\n",
              "      <td>0.495443</td>\n",
              "      <td>0.641672</td>\n",
              "      <td>0.166622</td>\n",
              "      <td>1.113050</td>\n",
              "      <td>-0.514069</td>\n",
              "      <td>-0.108101</td>\n",
              "      <td>0.563612</td>\n",
              "      <td>0.163444</td>\n",
              "      <td>1.052690</td>\n",
              "      <td>-0.263543</td>\n",
              "      <td>0.617824</td>\n",
              "      <td>-0.512447</td>\n",
              "      <td>-0.632055</td>\n",
              "      <td>-1.057613</td>\n",
              "      <td>0.164391</td>\n",
              "      <td>-0.715569</td>\n",
              "      <td>-0.747823</td>\n",
              "      <td>0.537585</td>\n",
              "      <td>-0.116105</td>\n",
              "      <td>-0.205757</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.007480</td>\n",
              "      <td>-0.007161</td>\n",
              "      <td>-0.003385</td>\n",
              "      <td>-0.007214</td>\n",
              "      <td>0.006967</td>\n",
              "      <td>-0.004896</td>\n",
              "      <td>-0.001689</td>\n",
              "      <td>0.002751</td>\n",
              "      <td>0.002773</td>\n",
              "      <td>0.003575</td>\n",
              "      <td>-0.002886</td>\n",
              "      <td>0.002113</td>\n",
              "      <td>0.003545</td>\n",
              "      <td>-0.006258</td>\n",
              "      <td>-0.005767</td>\n",
              "      <td>0.006883</td>\n",
              "      <td>0.005591</td>\n",
              "      <td>-0.001869</td>\n",
              "      <td>0.000745</td>\n",
              "      <td>0.002099</td>\n",
              "      <td>-0.000348</td>\n",
              "      <td>0.004043</td>\n",
              "      <td>-0.005000</td>\n",
              "      <td>-0.004704</td>\n",
              "      <td>0.002866</td>\n",
              "      <td>0.001804</td>\n",
              "      <td>0.003345</td>\n",
              "      <td>-0.002383</td>\n",
              "      <td>-0.002857</td>\n",
              "      <td>0.006740</td>\n",
              "      <td>0.002607</td>\n",
              "      <td>-0.007753</td>\n",
              "      <td>-0.007416</td>\n",
              "      <td>0.000145</td>\n",
              "      <td>-0.003446</td>\n",
              "      <td>0.002858</td>\n",
              "      <td>-0.001515</td>\n",
              "      <td>-0.006126</td>\n",
              "      <td>-0.000309</td>\n",
              "      <td>0.003714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1515524</td>\n",
              "      <td>0.007245</td>\n",
              "      <td>0.082101</td>\n",
              "      <td>0.480947</td>\n",
              "      <td>-0.876973</td>\n",
              "      <td>0.781694</td>\n",
              "      <td>0.141359</td>\n",
              "      <td>0.211259</td>\n",
              "      <td>0.517827</td>\n",
              "      <td>0.427663</td>\n",
              "      <td>-0.276976</td>\n",
              "      <td>-0.729186</td>\n",
              "      <td>0.689607</td>\n",
              "      <td>-0.346114</td>\n",
              "      <td>-0.488435</td>\n",
              "      <td>-0.107165</td>\n",
              "      <td>0.679893</td>\n",
              "      <td>-0.174342</td>\n",
              "      <td>-0.128430</td>\n",
              "      <td>-0.767647</td>\n",
              "      <td>-0.123611</td>\n",
              "      <td>-0.598480</td>\n",
              "      <td>0.562878</td>\n",
              "      <td>0.065125</td>\n",
              "      <td>-0.036792</td>\n",
              "      <td>-0.444272</td>\n",
              "      <td>0.644067</td>\n",
              "      <td>-0.392876</td>\n",
              "      <td>-0.186923</td>\n",
              "      <td>0.489799</td>\n",
              "      <td>-0.208338</td>\n",
              "      <td>-0.433074</td>\n",
              "      <td>0.113006</td>\n",
              "      <td>-0.877710</td>\n",
              "      <td>0.615984</td>\n",
              "      <td>-0.539929</td>\n",
              "      <td>-0.419203</td>\n",
              "      <td>-0.064564</td>\n",
              "      <td>0.162514</td>\n",
              "      <td>0.431589</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.005255</td>\n",
              "      <td>0.004866</td>\n",
              "      <td>-0.003454</td>\n",
              "      <td>-0.002683</td>\n",
              "      <td>-0.002002</td>\n",
              "      <td>0.004655</td>\n",
              "      <td>-0.007248</td>\n",
              "      <td>0.002210</td>\n",
              "      <td>-0.006482</td>\n",
              "      <td>0.004094</td>\n",
              "      <td>0.002389</td>\n",
              "      <td>0.005621</td>\n",
              "      <td>0.002380</td>\n",
              "      <td>-0.005766</td>\n",
              "      <td>0.001689</td>\n",
              "      <td>-0.000885</td>\n",
              "      <td>-0.001696</td>\n",
              "      <td>0.001604</td>\n",
              "      <td>-0.001521</td>\n",
              "      <td>-0.000896</td>\n",
              "      <td>0.007109</td>\n",
              "      <td>0.002954</td>\n",
              "      <td>-0.001629</td>\n",
              "      <td>-0.005539</td>\n",
              "      <td>-0.004413</td>\n",
              "      <td>-0.006690</td>\n",
              "      <td>-0.005202</td>\n",
              "      <td>-0.004700</td>\n",
              "      <td>0.003195</td>\n",
              "      <td>0.005895</td>\n",
              "      <td>0.002592</td>\n",
              "      <td>0.004534</td>\n",
              "      <td>-0.007363</td>\n",
              "      <td>0.005645</td>\n",
              "      <td>0.006898</td>\n",
              "      <td>0.000620</td>\n",
              "      <td>0.000940</td>\n",
              "      <td>-0.005028</td>\n",
              "      <td>0.006392</td>\n",
              "      <td>0.005439</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1606427</td>\n",
              "      <td>-0.975989</td>\n",
              "      <td>0.187017</td>\n",
              "      <td>-0.988799</td>\n",
              "      <td>1.767161</td>\n",
              "      <td>0.769870</td>\n",
              "      <td>-3.763815</td>\n",
              "      <td>0.132515</td>\n",
              "      <td>-2.558197</td>\n",
              "      <td>-0.826874</td>\n",
              "      <td>2.000220</td>\n",
              "      <td>-2.660278</td>\n",
              "      <td>1.376043</td>\n",
              "      <td>0.947726</td>\n",
              "      <td>0.849836</td>\n",
              "      <td>-0.564574</td>\n",
              "      <td>0.569658</td>\n",
              "      <td>1.704702</td>\n",
              "      <td>0.489982</td>\n",
              "      <td>0.451404</td>\n",
              "      <td>0.113303</td>\n",
              "      <td>-1.191751</td>\n",
              "      <td>1.018100</td>\n",
              "      <td>0.218787</td>\n",
              "      <td>-0.062853</td>\n",
              "      <td>1.258830</td>\n",
              "      <td>0.398644</td>\n",
              "      <td>-2.006275</td>\n",
              "      <td>-0.128674</td>\n",
              "      <td>0.522428</td>\n",
              "      <td>1.066818</td>\n",
              "      <td>-1.485506</td>\n",
              "      <td>0.981592</td>\n",
              "      <td>1.776454</td>\n",
              "      <td>-1.917482</td>\n",
              "      <td>0.974292</td>\n",
              "      <td>2.185344</td>\n",
              "      <td>-0.713491</td>\n",
              "      <td>0.710392</td>\n",
              "      <td>0.642025</td>\n",
              "      <td>...</td>\n",
              "      <td>0.006363</td>\n",
              "      <td>-0.006841</td>\n",
              "      <td>-0.002968</td>\n",
              "      <td>0.002493</td>\n",
              "      <td>-0.004403</td>\n",
              "      <td>-0.000058</td>\n",
              "      <td>0.003323</td>\n",
              "      <td>0.005582</td>\n",
              "      <td>-0.006956</td>\n",
              "      <td>0.007174</td>\n",
              "      <td>-0.001114</td>\n",
              "      <td>-0.000182</td>\n",
              "      <td>-0.006604</td>\n",
              "      <td>0.001045</td>\n",
              "      <td>0.004899</td>\n",
              "      <td>0.001815</td>\n",
              "      <td>0.007688</td>\n",
              "      <td>-0.005770</td>\n",
              "      <td>-0.004798</td>\n",
              "      <td>-0.002199</td>\n",
              "      <td>0.000803</td>\n",
              "      <td>0.006463</td>\n",
              "      <td>-0.003974</td>\n",
              "      <td>0.005718</td>\n",
              "      <td>-0.004282</td>\n",
              "      <td>-0.003011</td>\n",
              "      <td>-0.007387</td>\n",
              "      <td>-0.002689</td>\n",
              "      <td>-0.006804</td>\n",
              "      <td>0.003769</td>\n",
              "      <td>-0.006224</td>\n",
              "      <td>-0.006819</td>\n",
              "      <td>-0.003412</td>\n",
              "      <td>0.003950</td>\n",
              "      <td>-0.000678</td>\n",
              "      <td>0.002643</td>\n",
              "      <td>0.007413</td>\n",
              "      <td>-0.005458</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.004185</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 402 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   authorID     avg 0     avg 1  ...        61        62        63\n",
              "0   1036332 -0.670915 -0.359066  ...  0.006540  0.006745  0.007610\n",
              "1   1101850  0.174118 -0.418902  ...  0.000350 -0.001531 -0.003356\n",
              "2   1336878 -0.457677 -0.075856  ... -0.006126 -0.000309  0.003714\n",
              "3   1515524  0.007245  0.082101  ... -0.005028  0.006392  0.005439\n",
              "4   1606427 -0.975989  0.187017  ... -0.005458  0.000005  0.004185\n",
              "\n",
              "[5 rows x 402 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s86ggqtLE3Y1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40489d1d-d780-4c0d-a551-1e2ecdec35c4"
      },
      "source": [
        "### Individual node attributes \n",
        "G_nx = nx.read_edgelist('collaboration_network.edgelist', delimiter=' ', nodetype=int)\n",
        "\n",
        "print(\"computing centrality\")\n",
        "centrality=  nx.algorithms.centrality.degree_centrality(G_nx)\n",
        "print(\"computing page rank\")\n",
        "pagerank= nx.algorithms.link_analysis.pagerank_alg.pagerank(G_nx)\n",
        "df_full['degree_centrality']=df_full.apply(lambda row: centrality[row['authorID']], axis=1)\n",
        "df_full['pagerank']=df_full.apply(lambda row: pagerank[row['authorID']], axis=1)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "computing centrality\n",
            "computing page rank\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96Qwf_cSLJi3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        },
        "outputId": "d79b13cc-08c2-41de-d632-f4073cc55602"
      },
      "source": [
        "df_full.set_index(\"authorID\",inplace=True)\n",
        "df_full.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>avg 0</th>\n",
              "      <th>avg 1</th>\n",
              "      <th>avg 2</th>\n",
              "      <th>avg 3</th>\n",
              "      <th>avg 4</th>\n",
              "      <th>avg 5</th>\n",
              "      <th>avg 6</th>\n",
              "      <th>avg 7</th>\n",
              "      <th>avg 8</th>\n",
              "      <th>avg 9</th>\n",
              "      <th>avg 10</th>\n",
              "      <th>avg 11</th>\n",
              "      <th>avg 12</th>\n",
              "      <th>avg 13</th>\n",
              "      <th>avg 14</th>\n",
              "      <th>avg 15</th>\n",
              "      <th>avg 16</th>\n",
              "      <th>avg 17</th>\n",
              "      <th>avg 18</th>\n",
              "      <th>avg 19</th>\n",
              "      <th>avg 20</th>\n",
              "      <th>avg 21</th>\n",
              "      <th>avg 22</th>\n",
              "      <th>avg 23</th>\n",
              "      <th>avg 24</th>\n",
              "      <th>avg 25</th>\n",
              "      <th>avg 26</th>\n",
              "      <th>avg 27</th>\n",
              "      <th>avg 28</th>\n",
              "      <th>avg 29</th>\n",
              "      <th>avg 30</th>\n",
              "      <th>avg 31</th>\n",
              "      <th>avg 32</th>\n",
              "      <th>avg 33</th>\n",
              "      <th>avg 34</th>\n",
              "      <th>avg 35</th>\n",
              "      <th>avg 36</th>\n",
              "      <th>avg 37</th>\n",
              "      <th>avg 38</th>\n",
              "      <th>avg 39</th>\n",
              "      <th>...</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "      <th>55</th>\n",
              "      <th>56</th>\n",
              "      <th>57</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>degree_centrality</th>\n",
              "      <th>pagerank</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>authorID</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1036332</th>\n",
              "      <td>-0.670915</td>\n",
              "      <td>-0.359066</td>\n",
              "      <td>0.032644</td>\n",
              "      <td>0.001299</td>\n",
              "      <td>0.516241</td>\n",
              "      <td>-0.401442</td>\n",
              "      <td>-0.266719</td>\n",
              "      <td>-0.309738</td>\n",
              "      <td>-0.317130</td>\n",
              "      <td>0.317681</td>\n",
              "      <td>-0.330467</td>\n",
              "      <td>0.612775</td>\n",
              "      <td>-0.021864</td>\n",
              "      <td>-0.729525</td>\n",
              "      <td>-0.032873</td>\n",
              "      <td>0.296621</td>\n",
              "      <td>-0.117286</td>\n",
              "      <td>-0.256655</td>\n",
              "      <td>0.706496</td>\n",
              "      <td>-0.183294</td>\n",
              "      <td>-1.307308</td>\n",
              "      <td>0.519499</td>\n",
              "      <td>-0.647021</td>\n",
              "      <td>0.229126</td>\n",
              "      <td>0.537920</td>\n",
              "      <td>0.096529</td>\n",
              "      <td>0.169272</td>\n",
              "      <td>0.194358</td>\n",
              "      <td>0.874101</td>\n",
              "      <td>-0.068565</td>\n",
              "      <td>-0.514697</td>\n",
              "      <td>-0.346802</td>\n",
              "      <td>0.035213</td>\n",
              "      <td>-0.203666</td>\n",
              "      <td>-0.216070</td>\n",
              "      <td>-0.195695</td>\n",
              "      <td>-0.521439</td>\n",
              "      <td>0.021314</td>\n",
              "      <td>-0.365639</td>\n",
              "      <td>0.479228</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.000999</td>\n",
              "      <td>-0.002036</td>\n",
              "      <td>-0.004088</td>\n",
              "      <td>0.002935</td>\n",
              "      <td>0.007257</td>\n",
              "      <td>-0.001903</td>\n",
              "      <td>-0.006215</td>\n",
              "      <td>0.007679</td>\n",
              "      <td>0.000152</td>\n",
              "      <td>-0.000554</td>\n",
              "      <td>-0.000112</td>\n",
              "      <td>0.004722</td>\n",
              "      <td>0.002766</td>\n",
              "      <td>-0.003548</td>\n",
              "      <td>-0.007417</td>\n",
              "      <td>0.006346</td>\n",
              "      <td>0.002415</td>\n",
              "      <td>-0.005626</td>\n",
              "      <td>0.000391</td>\n",
              "      <td>0.000503</td>\n",
              "      <td>0.004809</td>\n",
              "      <td>0.000520</td>\n",
              "      <td>0.002058</td>\n",
              "      <td>-0.005602</td>\n",
              "      <td>0.006533</td>\n",
              "      <td>0.005513</td>\n",
              "      <td>-0.001609</td>\n",
              "      <td>0.005013</td>\n",
              "      <td>0.002052</td>\n",
              "      <td>-0.003057</td>\n",
              "      <td>-0.005553</td>\n",
              "      <td>0.006534</td>\n",
              "      <td>-0.006660</td>\n",
              "      <td>0.005388</td>\n",
              "      <td>0.002876</td>\n",
              "      <td>0.006540</td>\n",
              "      <td>0.006745</td>\n",
              "      <td>0.007610</td>\n",
              "      <td>0.000069</td>\n",
              "      <td>0.000015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1101850</th>\n",
              "      <td>0.174118</td>\n",
              "      <td>-0.418902</td>\n",
              "      <td>0.126907</td>\n",
              "      <td>-0.434909</td>\n",
              "      <td>0.057360</td>\n",
              "      <td>-0.896717</td>\n",
              "      <td>0.104900</td>\n",
              "      <td>-0.071798</td>\n",
              "      <td>0.058149</td>\n",
              "      <td>-0.162548</td>\n",
              "      <td>-0.446979</td>\n",
              "      <td>0.057747</td>\n",
              "      <td>0.534189</td>\n",
              "      <td>-0.334704</td>\n",
              "      <td>-0.095730</td>\n",
              "      <td>-0.196843</td>\n",
              "      <td>1.432983</td>\n",
              "      <td>0.573235</td>\n",
              "      <td>0.092543</td>\n",
              "      <td>-0.137559</td>\n",
              "      <td>-1.550742</td>\n",
              "      <td>-0.268918</td>\n",
              "      <td>-0.264333</td>\n",
              "      <td>0.146961</td>\n",
              "      <td>-0.152437</td>\n",
              "      <td>0.257579</td>\n",
              "      <td>0.177396</td>\n",
              "      <td>0.009939</td>\n",
              "      <td>0.566907</td>\n",
              "      <td>0.520262</td>\n",
              "      <td>-0.442662</td>\n",
              "      <td>-0.034450</td>\n",
              "      <td>-1.323437</td>\n",
              "      <td>0.642263</td>\n",
              "      <td>-0.510872</td>\n",
              "      <td>-0.719326</td>\n",
              "      <td>0.521323</td>\n",
              "      <td>0.450722</td>\n",
              "      <td>-0.078295</td>\n",
              "      <td>0.110865</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.000428</td>\n",
              "      <td>-0.001320</td>\n",
              "      <td>-0.003714</td>\n",
              "      <td>-0.004532</td>\n",
              "      <td>-0.006200</td>\n",
              "      <td>0.000738</td>\n",
              "      <td>0.001223</td>\n",
              "      <td>-0.004606</td>\n",
              "      <td>-0.004458</td>\n",
              "      <td>0.001919</td>\n",
              "      <td>0.002238</td>\n",
              "      <td>-0.006016</td>\n",
              "      <td>0.003875</td>\n",
              "      <td>0.002014</td>\n",
              "      <td>0.004882</td>\n",
              "      <td>0.000064</td>\n",
              "      <td>-0.006600</td>\n",
              "      <td>0.002346</td>\n",
              "      <td>0.005872</td>\n",
              "      <td>0.007262</td>\n",
              "      <td>-0.003594</td>\n",
              "      <td>-0.000818</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.003670</td>\n",
              "      <td>-0.003555</td>\n",
              "      <td>0.007660</td>\n",
              "      <td>0.005929</td>\n",
              "      <td>-0.004833</td>\n",
              "      <td>-0.001249</td>\n",
              "      <td>-0.004434</td>\n",
              "      <td>-0.004301</td>\n",
              "      <td>0.001798</td>\n",
              "      <td>0.002644</td>\n",
              "      <td>-0.000286</td>\n",
              "      <td>-0.001975</td>\n",
              "      <td>0.000350</td>\n",
              "      <td>-0.001531</td>\n",
              "      <td>-0.003356</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>0.000001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1336878</th>\n",
              "      <td>-0.457677</td>\n",
              "      <td>-0.075856</td>\n",
              "      <td>-0.706770</td>\n",
              "      <td>0.232660</td>\n",
              "      <td>-0.142577</td>\n",
              "      <td>0.157583</td>\n",
              "      <td>-0.518147</td>\n",
              "      <td>0.305940</td>\n",
              "      <td>-0.658546</td>\n",
              "      <td>0.092074</td>\n",
              "      <td>-0.138651</td>\n",
              "      <td>-0.315813</td>\n",
              "      <td>0.693731</td>\n",
              "      <td>-0.100878</td>\n",
              "      <td>-0.459728</td>\n",
              "      <td>0.764480</td>\n",
              "      <td>0.230007</td>\n",
              "      <td>0.411774</td>\n",
              "      <td>-0.775120</td>\n",
              "      <td>0.495443</td>\n",
              "      <td>0.641672</td>\n",
              "      <td>0.166622</td>\n",
              "      <td>1.113050</td>\n",
              "      <td>-0.514069</td>\n",
              "      <td>-0.108101</td>\n",
              "      <td>0.563612</td>\n",
              "      <td>0.163444</td>\n",
              "      <td>1.052690</td>\n",
              "      <td>-0.263543</td>\n",
              "      <td>0.617824</td>\n",
              "      <td>-0.512447</td>\n",
              "      <td>-0.632055</td>\n",
              "      <td>-1.057613</td>\n",
              "      <td>0.164391</td>\n",
              "      <td>-0.715569</td>\n",
              "      <td>-0.747823</td>\n",
              "      <td>0.537585</td>\n",
              "      <td>-0.116105</td>\n",
              "      <td>-0.205757</td>\n",
              "      <td>-0.311762</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.003385</td>\n",
              "      <td>-0.007214</td>\n",
              "      <td>0.006967</td>\n",
              "      <td>-0.004896</td>\n",
              "      <td>-0.001689</td>\n",
              "      <td>0.002751</td>\n",
              "      <td>0.002773</td>\n",
              "      <td>0.003575</td>\n",
              "      <td>-0.002886</td>\n",
              "      <td>0.002113</td>\n",
              "      <td>0.003545</td>\n",
              "      <td>-0.006258</td>\n",
              "      <td>-0.005767</td>\n",
              "      <td>0.006883</td>\n",
              "      <td>0.005591</td>\n",
              "      <td>-0.001869</td>\n",
              "      <td>0.000745</td>\n",
              "      <td>0.002099</td>\n",
              "      <td>-0.000348</td>\n",
              "      <td>0.004043</td>\n",
              "      <td>-0.005000</td>\n",
              "      <td>-0.004704</td>\n",
              "      <td>0.002866</td>\n",
              "      <td>0.001804</td>\n",
              "      <td>0.003345</td>\n",
              "      <td>-0.002383</td>\n",
              "      <td>-0.002857</td>\n",
              "      <td>0.006740</td>\n",
              "      <td>0.002607</td>\n",
              "      <td>-0.007753</td>\n",
              "      <td>-0.007416</td>\n",
              "      <td>0.000145</td>\n",
              "      <td>-0.003446</td>\n",
              "      <td>0.002858</td>\n",
              "      <td>-0.001515</td>\n",
              "      <td>-0.006126</td>\n",
              "      <td>-0.000309</td>\n",
              "      <td>0.003714</td>\n",
              "      <td>0.000463</td>\n",
              "      <td>0.000043</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1515524</th>\n",
              "      <td>0.007245</td>\n",
              "      <td>0.082101</td>\n",
              "      <td>0.480947</td>\n",
              "      <td>-0.876973</td>\n",
              "      <td>0.781694</td>\n",
              "      <td>0.141359</td>\n",
              "      <td>0.211259</td>\n",
              "      <td>0.517827</td>\n",
              "      <td>0.427663</td>\n",
              "      <td>-0.276976</td>\n",
              "      <td>-0.729186</td>\n",
              "      <td>0.689607</td>\n",
              "      <td>-0.346114</td>\n",
              "      <td>-0.488435</td>\n",
              "      <td>-0.107165</td>\n",
              "      <td>0.679893</td>\n",
              "      <td>-0.174342</td>\n",
              "      <td>-0.128430</td>\n",
              "      <td>-0.767647</td>\n",
              "      <td>-0.123611</td>\n",
              "      <td>-0.598480</td>\n",
              "      <td>0.562878</td>\n",
              "      <td>0.065125</td>\n",
              "      <td>-0.036792</td>\n",
              "      <td>-0.444272</td>\n",
              "      <td>0.644067</td>\n",
              "      <td>-0.392876</td>\n",
              "      <td>-0.186923</td>\n",
              "      <td>0.489799</td>\n",
              "      <td>-0.208338</td>\n",
              "      <td>-0.433074</td>\n",
              "      <td>0.113006</td>\n",
              "      <td>-0.877710</td>\n",
              "      <td>0.615984</td>\n",
              "      <td>-0.539929</td>\n",
              "      <td>-0.419203</td>\n",
              "      <td>-0.064564</td>\n",
              "      <td>0.162514</td>\n",
              "      <td>0.431589</td>\n",
              "      <td>0.370070</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.003454</td>\n",
              "      <td>-0.002683</td>\n",
              "      <td>-0.002002</td>\n",
              "      <td>0.004655</td>\n",
              "      <td>-0.007248</td>\n",
              "      <td>0.002210</td>\n",
              "      <td>-0.006482</td>\n",
              "      <td>0.004094</td>\n",
              "      <td>0.002389</td>\n",
              "      <td>0.005621</td>\n",
              "      <td>0.002380</td>\n",
              "      <td>-0.005766</td>\n",
              "      <td>0.001689</td>\n",
              "      <td>-0.000885</td>\n",
              "      <td>-0.001696</td>\n",
              "      <td>0.001604</td>\n",
              "      <td>-0.001521</td>\n",
              "      <td>-0.000896</td>\n",
              "      <td>0.007109</td>\n",
              "      <td>0.002954</td>\n",
              "      <td>-0.001629</td>\n",
              "      <td>-0.005539</td>\n",
              "      <td>-0.004413</td>\n",
              "      <td>-0.006690</td>\n",
              "      <td>-0.005202</td>\n",
              "      <td>-0.004700</td>\n",
              "      <td>0.003195</td>\n",
              "      <td>0.005895</td>\n",
              "      <td>0.002592</td>\n",
              "      <td>0.004534</td>\n",
              "      <td>-0.007363</td>\n",
              "      <td>0.005645</td>\n",
              "      <td>0.006898</td>\n",
              "      <td>0.000620</td>\n",
              "      <td>0.000940</td>\n",
              "      <td>-0.005028</td>\n",
              "      <td>0.006392</td>\n",
              "      <td>0.005439</td>\n",
              "      <td>0.000013</td>\n",
              "      <td>0.000002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1606427</th>\n",
              "      <td>-0.975989</td>\n",
              "      <td>0.187017</td>\n",
              "      <td>-0.988799</td>\n",
              "      <td>1.767161</td>\n",
              "      <td>0.769870</td>\n",
              "      <td>-3.763815</td>\n",
              "      <td>0.132515</td>\n",
              "      <td>-2.558197</td>\n",
              "      <td>-0.826874</td>\n",
              "      <td>2.000220</td>\n",
              "      <td>-2.660278</td>\n",
              "      <td>1.376043</td>\n",
              "      <td>0.947726</td>\n",
              "      <td>0.849836</td>\n",
              "      <td>-0.564574</td>\n",
              "      <td>0.569658</td>\n",
              "      <td>1.704702</td>\n",
              "      <td>0.489982</td>\n",
              "      <td>0.451404</td>\n",
              "      <td>0.113303</td>\n",
              "      <td>-1.191751</td>\n",
              "      <td>1.018100</td>\n",
              "      <td>0.218787</td>\n",
              "      <td>-0.062853</td>\n",
              "      <td>1.258830</td>\n",
              "      <td>0.398644</td>\n",
              "      <td>-2.006275</td>\n",
              "      <td>-0.128674</td>\n",
              "      <td>0.522428</td>\n",
              "      <td>1.066818</td>\n",
              "      <td>-1.485506</td>\n",
              "      <td>0.981592</td>\n",
              "      <td>1.776454</td>\n",
              "      <td>-1.917482</td>\n",
              "      <td>0.974292</td>\n",
              "      <td>2.185344</td>\n",
              "      <td>-0.713491</td>\n",
              "      <td>0.710392</td>\n",
              "      <td>0.642025</td>\n",
              "      <td>-1.820679</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.002968</td>\n",
              "      <td>0.002493</td>\n",
              "      <td>-0.004403</td>\n",
              "      <td>-0.000058</td>\n",
              "      <td>0.003323</td>\n",
              "      <td>0.005582</td>\n",
              "      <td>-0.006956</td>\n",
              "      <td>0.007174</td>\n",
              "      <td>-0.001114</td>\n",
              "      <td>-0.000182</td>\n",
              "      <td>-0.006604</td>\n",
              "      <td>0.001045</td>\n",
              "      <td>0.004899</td>\n",
              "      <td>0.001815</td>\n",
              "      <td>0.007688</td>\n",
              "      <td>-0.005770</td>\n",
              "      <td>-0.004798</td>\n",
              "      <td>-0.002199</td>\n",
              "      <td>0.000803</td>\n",
              "      <td>0.006463</td>\n",
              "      <td>-0.003974</td>\n",
              "      <td>0.005718</td>\n",
              "      <td>-0.004282</td>\n",
              "      <td>-0.003011</td>\n",
              "      <td>-0.007387</td>\n",
              "      <td>-0.002689</td>\n",
              "      <td>-0.006804</td>\n",
              "      <td>0.003769</td>\n",
              "      <td>-0.006224</td>\n",
              "      <td>-0.006819</td>\n",
              "      <td>-0.003412</td>\n",
              "      <td>0.003950</td>\n",
              "      <td>-0.000678</td>\n",
              "      <td>0.002643</td>\n",
              "      <td>0.007413</td>\n",
              "      <td>-0.005458</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.004185</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>0.000004</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 403 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             avg 0     avg 1     avg 2  ...        63  degree_centrality  pagerank\n",
              "authorID                                ...                                       \n",
              "1036332  -0.670915 -0.359066  0.032644  ...  0.007610           0.000069  0.000015\n",
              "1101850   0.174118 -0.418902  0.126907  ... -0.003356           0.000009  0.000001\n",
              "1336878  -0.457677 -0.075856 -0.706770  ...  0.003714           0.000463  0.000043\n",
              "1515524   0.007245  0.082101  0.480947  ...  0.005439           0.000013  0.000002\n",
              "1606427  -0.975989  0.187017 -0.988799  ...  0.004185           0.000009  0.000004\n",
              "\n",
              "[5 rows x 403 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfDyrcKINwJI"
      },
      "source": [
        "train_data = pd.read_csv('train.csv', dtype={'authorID': np.int64, 'h_index': np.float32}).set_index(\"authorID\")\n",
        "test_data  = pd.read_csv('test.csv', dtype={'authorID': np.int64})\n",
        "\n",
        "test_auths = test_data[\"authorID\"]\n",
        "train_auths = train_data.index\n",
        "train_auths,holdout_auths = model_selection.train_test_split(train_auths,test_size=0.3)\n",
        "\n",
        "SC = StandardScaler()\n",
        "SC = SC.fit(df_full)\n",
        "df_full_sc  = pd.DataFrame(SC.transform(df_full.values), columns=df_full.columns, index=df_full.index)\n",
        "\n",
        "\n",
        "X_train = df_full_sc.loc[train_auths]\n",
        "y_train = train_data.loc[train_auths]['h_index'].values\n",
        "\n",
        "X_holdout = df_full_sc.loc[holdout_auths]\n",
        "y_holdout = train_data.loc[holdout_auths]['h_index'].values\n",
        "\n",
        "X_test = df_full.loc[test_auths]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUsL4EGNEmW6"
      },
      "source": [
        "# Deep Learning Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAhMu8AucZui",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "6bcc1773-4818-47e7-d153-b1ec9f358607"
      },
      "source": [
        "def np_boxcox(y):\n",
        "  return (np.power(y,ld)-1)/ld\n",
        "def np_inv_boxcox(y):\n",
        "  return np.exp(np.log(ld *y + 1 )/ld)\n",
        "\n",
        "def real_mae(y_true,y_pred):\n",
        "      return K.mean(K.abs(tf.math.round(y_pred)-y_true))\n",
        "\n",
        "def inv_boxcox(y):\n",
        "  return K.exp(K.log(ld * y + 1 )/ld)\n",
        "\n",
        "def scaled_mae(y_true,y_pred):\n",
        "  \"\"\" To use with scaled targets\"\"\"\n",
        "  y_true = inv_boxcox(y_true)\n",
        "  y_pred = inv_boxcox(y_pred)\n",
        "  return real_mae(y_true,y_pred)\n",
        "def LR(x):\n",
        "    return leaky_relu(x, alpha=0.2)\n",
        "\n",
        "\n",
        "train_data = pd.read_csv('train.csv', dtype={'authorID': np.int64, 'h_index': np.float32}).set_index(\"authorID\")\n",
        "y_sc,ld= boxcox(train_data[\"h_index\"].values)\n",
        "y_train_sc = np_boxcox(y_train)\n",
        "y_holdout_sc = np_boxcox(y_holdout)\n",
        "plt.hist(y_sc,bins=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([3931., 2805., 2104., 4483., 2677., 3257., 2180., 1202.,  458.,\n",
              "          27.]),\n",
              " array([0.        , 0.42177835, 0.8435567 , 1.2653351 , 1.6871134 ,\n",
              "        2.1088917 , 2.5306702 , 2.9524484 , 3.3742268 , 3.796005  ,\n",
              "        4.2177835 ], dtype=float32),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAANjklEQVR4nO3db4hdd53H8ffHpFpB1lQ7lG4SdgqGXeKCrYQ2S59Ii23aiOkDlcquBgnkSRcquLjpPin+KaRPWldYhWCC0XWNWRVaWqGEtiKCtk1t7dpkS2drShOqiSatFrFL6ncfzC/dazp/m5l7J/N7v2CYc37n3Ht+95K853DvmTupKiRJfXjLqCcgSRoeoy9JHTH6ktQRoy9JHTH6ktSRlaOewEwuvvjiGh8fH/U0JOm88vjjj/+mqsam2rakoz8+Ps7BgwdHPQ1JOq8keX66bb68I0kdMfqS1BGjL0kdMfqS1BGjL0kdMfqS1BGjL0kdMfqS1BGjL0kdWdK/kavzx/iO+0dy3CM7N4/kuNL5yjN9SeqI0Zekjhh9SeqI0Zekjhh9SeqI0Zekjhh9SeqI0Zekjhh9SeqI0Zekjhh9SeqI0Zekjhh9SeqI0Zekjhh9SeqI0Zekjhh9SeqI0Zekjsw5+klWJHkiyX1t/bIkjySZSPKdJG9t429r6xNt+/jAfdzWxp9Jcv1CPxhJ0szmc6Z/K3B4YP1O4O6qeg9wCtjWxrcBp9r43W0/kqwHbgbeC2wCvpJkxblNX5I0H3OKfpI1wGbga209wDXAd9sue4Gb2vKWtk7bfm3bfwuwr6perapfAhPAlQvxICRJczPXM/0vAZ8F/tTW3w28VFWn2/pRYHVbXg28ANC2v9z2f318itu8Lsn2JAeTHDxx4sQ8HookaTazRj/Jh4DjVfX4EOZDVe2qqg1VtWFsbGwYh5Skbqycwz5XAx9OciNwIfAXwL8Cq5KsbGfza4Bjbf9jwFrgaJKVwDuB3w6MnzF4G0nSEMwa/aq6DbgNIMkHgH+qqr9P8p/AR4B9wFbgnnaTe9v6T9r2h6qqktwL/EeSu4C/BNYBjy7sw/lz4zvuX8y7n9aRnZtHclxJms1czvSn88/AviRfBJ4Adrfx3cA3k0wAJ5m8YoeqejrJfuAQcBq4papeO4fjS5LmaV7Rr6ofAj9sy88xxdU3VfVH4KPT3P4O4I75TlKStDD8jVxJ6ojRl6SOGH1J6ojRl6SOGH1J6ojRl6SOGH1J6ojRl6SOGH1J6ojRl6SOGH1J6ojRl6SOGH1J6ojRl6SOGH1J6ojRl6SOGH1J6ojRl6SOGH1J6ojRl6SOGH1J6ojRl6SOGH1J6ojRl6SOGH1J6ojRl6SOGH1J6ojRl6SOrBz1BKTz1fiO+0dy3CM7N4/kuFoePNOXpI4YfUnqiNGXpI4YfUnqiNGXpI4YfUnqiNGXpI4YfUnqyKzRT3JhkkeT/DzJ00k+18YvS/JIkokk30ny1jb+trY+0baPD9zXbW38mSTXL9aDkiRNbS5n+q8C11TV+4DLgU1JNgJ3AndX1XuAU8C2tv824FQbv7vtR5L1wM3Ae4FNwFeSrFjIByNJmtms0a9Jr7TVC9pXAdcA323je4Gb2vKWtk7bfm2StPF9VfVqVf0SmACuXJBHIUmakzm9pp9kRZIngePAAeB/gJeq6nTb5Siwui2vBl4AaNtfBt49OD7FbQaPtT3JwSQHT5w4Mf9HJEma1pyiX1WvVdXlwBomz87/ZrEmVFW7qmpDVW0YGxtbrMNIUpfmdfVOVb0EPAz8HbAqyZlP6VwDHGvLx4C1AG37O4HfDo5PcRtJ0hDM5eqdsSSr2vLbgQ8Ch5mM/0fabluBe9ryvW2dtv2hqqo2fnO7uucyYB3w6EI9EEnS7ObyefqXAnvblTZvAfZX1X1JDgH7knwReALY3fbfDXwzyQRwkskrdqiqp5PsBw4Bp4Fbquq1hX04kqSZzBr9qnoKuGKK8eeY4uqbqvoj8NFp7usO4I75T1OStBD8jVxJ6ojRl6SOGH1J6oh/GH0R+AezJS1VnulLUkeMviR1xOhLUkeMviR1xOhLUke8ekfntVFdKSWdrzzTl6SOGH1J6ojRl6SOGH1J6ojRl6SOGH1J6ojRl6SOGH1J6ojRl6SOGH1J6ojRl6SOGH1J6ojRl6SOGH1J6ojRl6SOGH1J6ojRl6SOGH1J6ojRl6SOGH1J6ojRl6SOGH1J6sjKUU9A0vyM77h/ZMc+snPzyI6theGZviR1xOhLUkeMviR1xNf0l5FRvtYr6fww65l+krVJHk5yKMnTSW5t4+9KciDJs+37RW08Sb6cZCLJU0neP3BfW9v+zybZungPS5I0lbm8vHMa+ExVrQc2ArckWQ/sAB6sqnXAg20d4AZgXfvaDnwVJn9IALcDVwFXAref+UEhSRqOWaNfVS9W1c/a8u+Bw8BqYAuwt+22F7ipLW8BvlGTfgqsSnIpcD1woKpOVtUp4ACwaUEfjSRpRvN6IzfJOHAF8AhwSVW92Db9CrikLa8GXhi42dE2Nt342cfYnuRgkoMnTpyYz/QkSbOYc/STvAP4HvDpqvrd4LaqKqAWYkJVtauqNlTVhrGxsYW4S0lSM6foJ7mAyeB/q6q+34Z/3V62oX0/3saPAWsHbr6mjU03LkkakrlcvRNgN3C4qu4a2HQvcOYKnK3APQPjn2xX8WwEXm4vAz0AXJfkovYG7nVtTJI0JHO5Tv9q4BPAfyV5so39C7AT2J9kG/A88LG27QfAjcAE8AfgUwBVdTLJF4DH2n6fr6qTC/IoJElzMmv0q+rHQKbZfO0U+xdwyzT3tQfYM58JSpIWjh/DIEkdMfqS1BGjL0kdMfqS1BGjL0kdMfqS1BGjL0kdMfqS1BGjL0kdMfqS1BGjL0kdMfqS1BGjL0kdMfqS1BGjL0kdMfqS1BGjL0kdMfqS1BGjL0kdMfqS1BGjL0kdMfqS1BGjL0kdMfqS1BGjL0kdMfqS1BGjL0kdMfqS1BGjL0kdMfqS1BGjL0kdWTnqCUg6f4zvuH8kxz2yc/NIjrsceaYvSR0x+pLUEaMvSR0x+pLUEaMvSR2ZNfpJ9iQ5nuQXA2PvSnIgybPt+0VtPEm+nGQiyVNJ3j9wm61t/2eTbF2chyNJmslczvS/Dmw6a2wH8GBVrQMebOsANwDr2td24Ksw+UMCuB24CrgSuP3MDwpJ0vDMGv2q+hFw8qzhLcDetrwXuGlg/Bs16afAqiSXAtcDB6rqZFWdAg7wxh8kkqRF9mZf07+kql5sy78CLmnLq4EXBvY72samG5ckDdE5v5FbVQXUAswFgCTbkxxMcvDEiRMLdbeSJN589H/dXrahfT/exo8Bawf2W9PGpht/g6raVVUbqmrD2NjYm5yeJGkqbzb69wJnrsDZCtwzMP7JdhXPRuDl9jLQA8B1SS5qb+Be18YkSUM06weuJfk28AHg4iRHmbwKZyewP8k24HngY233HwA3AhPAH4BPAVTVySRfAB5r+32+qs5+c1iStMhmjX5VfXyaTddOsW8Bt0xzP3uAPfOanSRpQfkbuZLUEaMvSR0x+pLUEaMvSR0x+pLUEaMvSR0x+pLUEaMvSR0x+pLUEaMvSR0x+pLUEaMvSR0x+pLUEaMvSR0x+pLUEaMvSR0x+pLUEaMvSR0x+pLUEaMvSR2Z9Q+jS9Koje+4fyTHPbJz80iOu5g805ekjhh9SeqI0Zekjhh9SeqI0Zekjhh9SeqI0Zekjhh9SeqI0Zekjhh9SeqI0Zekjhh9SeqI0Zekjhh9SeqI0Zekjhh9SeqI0Zekjgw9+kk2JXkmyUSSHcM+viT1bKjRT7IC+DfgBmA98PEk64c5B0nq2bD/Ru6VwERVPQeQZB+wBTg05HlI0qxG9bd5YfH+Pu+wo78aeGFg/Shw1eAOSbYD29vqK0meOYfjXQz85hxuv9z5/MzM52dmPj8zO6fnJ3ee07H/aroNw47+rKpqF7BrIe4rycGq2rAQ97Uc+fzMzOdnZj4/M1uqz8+w38g9BqwdWF/TxiRJQzDs6D8GrEtyWZK3AjcD9w55DpLUraG+vFNVp5P8I/AAsALYU1VPL+IhF+RlomXM52dmPj8z8/mZ2ZJ8flJVo56DJGlI/I1cSeqI0ZekjizL6PtRDzNLsifJ8SS/GPVclpoka5M8nORQkqeT3DrqOS01SS5M8miSn7fn6HOjntNSlGRFkieS3DfquQxadtH3ox7m5OvAplFPYok6DXymqtYDG4Fb/PfzBq8C11TV+4DLgU1JNo54TkvRrcDhUU/ibMsu+gx81ENV/S9w5qMe1FTVj4CTo57HUlRVL1bVz9ry75n8T7t6tLNaWmrSK231gvblFSEDkqwBNgNfG/VczrYcoz/VRz34n1bzlmQcuAJ4ZLQzWXraSxdPAseBA1Xlc/TnvgR8FvjTqCdytuUYfemcJXkH8D3g01X1u1HPZ6mpqteq6nImf6v+yiR/O+o5LRVJPgQcr6rHRz2XqSzH6PtRDzonSS5gMvjfqqrvj3o+S1lVvQQ8jO8RDboa+HCSI0y+vHxNkn8f7ZT+33KMvh/1oDctSYDdwOGqumvU81mKkowlWdWW3w58EPjv0c5q6aiq26pqTVWNM9mfh6rqH0Y8rdctu+hX1WngzEc9HAb2L/JHPZx3knwb+Anw10mOJtk26jktIVcDn2Dy7OzJ9nXjqCe1xFwKPJzkKSZPsg5U1ZK6LFHT82MYJKkjy+5MX5I0PaMvSR0x+pLUEaMvSR0x+pLUEaMvSR0x+pLUkf8D3ws7k5TRjd8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1ogAC7q89jR"
      },
      "source": [
        "class ResDense(tf.keras.Model):\n",
        "\n",
        "  def __init__(self,size,activation,dropout_rate,bn=True,residual=True,*args):\n",
        "    super(ResDense, self).__init__()\n",
        "    \n",
        "    \n",
        "    self.bn = bn\n",
        "    self.residual  = residual\n",
        "\n",
        "    self.dense= Dense(size,activation=activation,*args)\n",
        "    self.bn=BatchNormalization()\n",
        "    self.dropout = Dropout(dropout_rate)\n",
        "    \n",
        "  def call(self,input,training=False):\n",
        "    output = self.dense(input)\n",
        "    output = self.dropout(output)\n",
        "    if self.bn: output = self.bn(output)  \n",
        "    if self.residual : output += input\n",
        "\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVuZ7-Dw1S4e"
      },
      "source": [
        "class Net(tf.keras.Model):\r\n",
        "  \"\"\"\r\n",
        "  This model looks at each block of features separately \r\n",
        "  \"\"\"\r\n",
        "  def __init__(self):\r\n",
        "    super(Net, self).__init__()\r\n",
        "    self.dense11= Dense(64, activation=relu)  # LINE\r\n",
        "    self.dense12= Dense(8, activation=relu)\r\n",
        "\r\n",
        "    self.dense21= Dense(32, activation=relu) # Deep Walk\r\n",
        "    self.dense22= Dense(8, activation=relu)\r\n",
        "\r\n",
        "\r\n",
        "    self.dense31= Dense(32, activation=relu) # Keywords embedding\r\n",
        "    self.dense32= Dense(8, activation=relu) \r\n",
        "\r\n",
        "\r\n",
        "    self.dense41= Dense(128, activation=relu) # mean Doc2vec\r\n",
        "    self.dense42= Dense(8, activation=relu)\r\n",
        "\r\n",
        "    self.dense51= Dense(128, activation=relu) # std Doc2vec\r\n",
        "    self.dense52= Dense(8, activation=relu)\r\n",
        "\r\n",
        "\r\n",
        "    self.dense61= Dense(128, activation=relu)  # max Doc2vec\r\n",
        "    self.dense62= Dense(8, activation=relu)\r\n",
        "\r\n",
        "    self.dense71= Dense(128, activation=relu) # min Doc2vec\r\n",
        "    self.dense72= Dense(8, activation=relu)\r\n",
        "\r\n",
        "    self.dense81= Dense(64, activation=relu) # graph features\r\n",
        "    self.dense82= Dense(16, activation=relu)\r\n",
        "\r\n",
        "\r\n",
        "    self.final_dense = Sequential([\r\n",
        "        Dense(128, activation=relu),\r\n",
        "        Dense(64,activation=relu),\r\n",
        "        Dense(1)])\r\n",
        "\r\n",
        "\r\n",
        "  def call(self,input):\r\n",
        "\r\n",
        "\r\n",
        "    output1 = self.dense11(input[:,:128])  # LINE\r\n",
        "    output1 = self.dense12(output1)\r\n",
        "\r\n",
        "\r\n",
        "    output2 = self.dense21(input[:,128:192])  # Deep Walk\r\n",
        "    output2 = self.dense22(output2)\r\n",
        "\r\n",
        "\r\n",
        "    output3 = self.dense31(input[:,192:256]) # Keywords embedding\r\n",
        "    output3 = self.dense32(output3)\r\n",
        "\r\n",
        "\r\n",
        "    output4 = self.dense41(input[:,256:512]) # mean Doc2vec\r\n",
        "    output4 = self.dense42(output4)\r\n",
        "\r\n",
        "    output5 = self.dense51(input[:,512:768]) # std Doc2vec\r\n",
        "    output5 = self.dense52(output5)\r\n",
        "\r\n",
        "\r\n",
        "    output6 = self.dense61(input[:,768:1024]) # max Doc2vec\r\n",
        "    output6 = self.dense62(output6)\r\n",
        "\r\n",
        "    output7 = self.dense71(input[:,1024:1280]) # min Doc2vec\r\n",
        "    output7 = self.dense72(output7)\r\n",
        "\r\n",
        "    output8 = self.dense81(input[:,1280:]) # graph features\r\n",
        "    output8 = self.dense82(output8)\r\n",
        "\r\n",
        "    output = self.final_dense(tf.concat([output1,output2,output4,output3,output5,output6,output7,output8], 1))\r\n",
        "\r\n",
        "    return output\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4Q-Yci51nKy"
      },
      "source": [
        "class Attention_Net(tf.keras.Model):\r\n",
        "\r\n",
        "  \"\"\"\r\n",
        "  Model with attention mechanism\r\n",
        "\r\n",
        "  \"\"\"\r\n",
        "\r\n",
        "  def __init__(self):\r\n",
        "    super(Attention_Net, self).__init__()\r\n",
        "    self.dense1 =Sequential([Dense(128, activation=relu),\r\n",
        "                             Dense(64, activation=relu),\r\n",
        "                             Dense(32, activation=relu),\r\n",
        "                             Dense(16, activation=relu),\r\n",
        "                             Dense(1, activation=relu)]) \r\n",
        "    self.dense2 = Sequential([Dense(128, activation=relu),\r\n",
        "                             Dense(64, activation=relu),\r\n",
        "                             Dense(32, activation=relu),\r\n",
        "                             Dense(16, activation=relu),\r\n",
        "                             Dense(1, activation=relu)]) \r\n",
        "\r\n",
        "\r\n",
        "    self.attention = Sequential([Dense(128, activation=relu),\r\n",
        "                                 Dense(2,activation='softmax')]) \r\n",
        "\r\n",
        "\r\n",
        "  def call(self,input):\r\n",
        "\r\n",
        "\r\n",
        "    output1 = self.dense1(tf.concat([input[:,:128] , input[:,192:256],input[:,768:1024],input[:,1024:1280], input[:,1280:]],1)) # LINE + Keywords embedding + min Doc2vec +max Doc2vec + graph features\r\n",
        "\r\n",
        "    output2 = self.dense2(tf.concat([input[:,128:192],input[:,256:512],input[:,512:768], input[:,1280:]],axis=1))  # Deep walk  + mean Doc2vec +std Doc2vec +graph features\r\n",
        "\r\n",
        "    attention = self.attention(input)\r\n",
        "\r\n",
        "    output = tf.multiply(tf.stack([output1,output2],axis=2), tf.expand_dims(attention , axis=1))\r\n",
        "\r\n",
        "    output = tf.reduce_sum(output, axis=[1,2])\r\n",
        "\r\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgQJmp3nc3Qh"
      },
      "source": [
        "def train_model(scaled_targets=False,scaled_X=False,PCA=False,\n",
        "                batch_size=200,n_layers=10,bn=True,residual=True,dropout=0.15,activation='lr'):\n",
        "\n",
        "        \n",
        "  \n",
        "        if PCA : X = X_train_pca\n",
        "        else : X = X_train\n",
        "\n",
        "        patience = 100\n",
        "\n",
        "        n_cols = X.shape[1]\n",
        "      \n",
        "        model = Sequential()\n",
        "        #model.add(Dropout(dropout))\n",
        "        for i in range(n_layers):\n",
        "          model.add(ResDense(n_cols, activation=activation,dropout_rate=dropout,bn=bn,residual=residual))        \n",
        "        \n",
        "        model.add(Dense(300,activation=activation)) \n",
        "        model.add(Dropout(dropout))\n",
        "        model.add(BatchNormalization())\n",
        "\n",
        "        model.add(Dense(1,activation=activation))\n",
        "\n",
        "        # Optimizer\n",
        "\n",
        "        opt = NovoGrad(lr=lr)\n",
        "        opt = tfa.optimizers.Lookahead(opt) ##WRAP OPTIMIZER WITH LookAhead layer \n",
        "\n",
        "        #########################################\n",
        "        if scaled_targets : \n",
        "          Y = y_train_sc\n",
        "          model.compile(optimizer=opt, loss=\"mae\",metrics=[scaled_mae])\n",
        "          # train model\n",
        "          ##Early stopping\n",
        "          es = tf.keras.callbacks.EarlyStopping(monitor='val_scaled_mae',restore_best_weights=True, patience=200,min_delta=0.05,verbose=2)\n",
        "          reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_scaled_mae', factor=0.3, patience=100, mode='auto', min_delta=0.1,verbose=2)\n",
        "          history=model.fit(X,Y,batch_size=batch_size,validation_split=0.2, epochs=1000,callbacks=[es,reduce_lr])\n",
        "        \n",
        "        else :\n",
        "\n",
        "          Y = y_train\n",
        "          model.compile(optimizer=opt, loss=\"mae\",metrics=[real_mae])\n",
        "          # # train model\n",
        "          es = tf.keras.callbacks.EarlyStopping(monitor='val_real_mae',restore_best_weights=True, patience=patience*3,min_delta=0.1,verbose=1)\n",
        "          reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='real_mae', factor=0.3, patience=patience, mode='auto', min_delta=0.1,verbose=1)\n",
        "          history=model.fit(X,Y,batch_size=batch_size,validation_split=0.2, epochs=1000,callbacks=[es,reduce_lr])\n",
        "\n",
        "\n",
        "        return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMJnmDDU7smP"
      },
      "source": [
        "#pretrained_model = train_model(PCA=False,scaled_targets=False,dropout=0.0,batch_size=150,n_layers=10,residual=True,bn=True)      \n",
        "# holdout_preds = pretrained_model.predict(X_holdout)\n",
        "# #real_mae(inv_boxcox(holdout_preds),y_holdout)\n",
        "# real_mae(holdout_preds,y_holdout)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ojQP8uapt83"
      },
      "source": [
        "# Boost performance with APPNP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SInPzvd7p65U"
      },
      "source": [
        "n_cols = df_full_sc.shape[1]\n",
        "G_nx = nx.read_edgelist('collaboration_network.edgelist', delimiter=' ', nodetype=int)\n",
        "G = StellarGraph.from_networkx(G_nx,node_features=df_full_sc, node_type_default=\"author\", edge_type_default=\"cites\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nl47xttVuwO-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54b45b0e-bbd6-442a-85c5-db393b71194d"
      },
      "source": [
        "targets = pd.read_csv('train.csv', dtype={'authorID': np.int64, 'h_index': np.float32})\n",
        "targets.set_index(\"authorID\",inplace=True)\n",
        "\n",
        "median = np.median(targets['h_index'].values)\n",
        "targets_shifted = pd.DataFrame(targets['h_index'].values - median, columns=['h_index'], index=targets.index)\n",
        "targets_sc = pd.DataFrame(y_sc, columns=['h_index'], index=targets.index)\n",
        "\n",
        "# Use scikit-learn to compute training and test sets\n",
        "\n",
        "train_targets, val_targets = model_selection.train_test_split(targets, test_size=0.05)\n",
        "train_targets_sc, val_targets_sc = model_selection.train_test_split(targets_sc, test_size=0.05)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "generator = FullBatchNodeGenerator(G, method=\"gcn\", sparse=True)\n",
        "\n",
        "train_gen = generator.flow(train_targets.index, train_targets)\n",
        "val_gen = generator.flow(val_targets.index, val_targets)\n",
        "\n",
        "train_gen_sc = generator.flow(train_targets.index, train_targets_sc)\n",
        "val_gen_sc = generator.flow(val_targets.index, val_targets_sc)\n",
        "\n",
        "\n",
        "test_gen = generator.flow(test_data[\"authorID\"])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using GCN (local pooling) filters...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADHk-OiyTl2u"
      },
      "source": [
        "def fit_model_params(lr,n_layers=4,tp=0.5,dropout=0.15,bn=True,residual=True,lookahead=True,optimizer='Novograd',activation='elu',scaled=False):\n",
        "\n",
        "      custom_appnp = APPNP(\n",
        "        layer_sizes=[200],\n",
        "        activations=[LR],\n",
        "        generator=generator,\n",
        "        teleport_probability=tp)\n",
        "\n",
        "      feature_layers = []\n",
        "\n",
        "      model = Sequential()\n",
        "\n",
        "      for i in range(n_layers):\n",
        "        model.add((ResDense(n_cols, activation=LR,dropout_rate=dropout,bn=bn)))\n",
        "      \n",
        "      model.add(Dense(100,activation=LR)) ## should be elu\n",
        "      model.add(Dropout(dropout))\n",
        "      if bn : model.add(BatchNormalization())\n",
        "      model.add(Dense(1,activation=LR)) ## should be elu\n",
        "\n",
        "\n",
        "\n",
        "      custom_appnp._feature_layers = [model]\n",
        "\n",
        "      x_inp, x_out = custom_appnp.in_out_tensors()\n",
        "\n",
        "      custom_appnp_model = tf.keras.models.Model(inputs=x_inp, outputs=x_out)\n",
        "      if optimizer == 'Novograd':opt = NovoGrad(lr=lr)\n",
        "      else : opt=tfa.optimizers.LAMB(learning_rate=lr)\n",
        "      if lookahead: opt = tfa.optimizers.Lookahead(opt) ##WRAP OPTIMIZER WITH LookAhead layer \n",
        "\n",
        "      es,lr_schedule = callbacks()\n",
        "      \n",
        "      if scaled :\n",
        "          custom_appnp_model.compile(loss=\"mae\",metrics=[sclaed_mae],optimizer=opt)\n",
        "          history=  custom_appnp_model.fit(train_gen_sc,epochs=2500,validation_data=val_gen_sc,shuffle=False,callbacks =[es,lr_schedule,NeptuneMonitor()])  # this should be False, since shuffling data means shuffling the whole graph                                     \n",
        "      \n",
        "      else : \n",
        "          custom_appnp_model.compile(loss=\"mae\",metrics=[real_mae],optimizer=opt)\n",
        "          history=  custom_appnp_model.fit(train_gen,epochs=2500,validation_data=val_gen,shuffle=False,callbacks=[es,lr_schedule,NeptuneMonitor()])  # this should be False, since shuffling data means shuffling the whole graph                                     \n",
        "\n",
        "      return history\n",
        "\n",
        "\n",
        "def callbacks():\n",
        "  reduce_lr = ReduceLROnPlateau(monitor='val_real_mae', factor=0.5,mode = \"min\",patience=75, min_lr=1e-6,min_delta=0.066,verbose=1)\n",
        "  es = tf.keras.callbacks.EarlyStopping(monitor='val_real_mae',restore_best_weights=True, patience=200,min_delta=0.066,verbose=1)\n",
        "\n",
        "  return [reduce_lr,es]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alEdpnM3FjtO"
      },
      "source": [
        "# Optuna for HP optimizaiton\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZJ3o4DAFxo6"
      },
      "source": [
        "def objective(trial,input_size = df_full_sc.shape[0],):\n",
        "      # We optimize the numbers of layers and their units.\n",
        "\n",
        "      params = {\n",
        "        'n_layers':trial.suggest_int(\"n_layers\", 1,8),\n",
        "        'tp':trial.suggest_uniform('tp',0.75,1),\n",
        "        'scaled':False,\n",
        "        'bn':trial.suggest_categorical('bn',[False,True]),\n",
        "        'dropout':trial.suggest_uniform('dropout',0.0,0.6),\n",
        "        'lr':trial.suggest_uniform('lr',1e-3,9e-2),\n",
        "        'optimizer':trial.suggest_categorical('optmizer',['Novograd','LAMB']),\n",
        "        'lookahead':trial.suggest_categorical('lookahead',[False,True]),\n",
        "        'activation':trial.suggest_categorical('activation',['relu','lr','elu']),\n",
        "        'residual': False,\n",
        "      }\n",
        "      print(\"***************************params*******************\\n\",params)\n",
        "      history = fit_model_params(**params)\n",
        "   \n",
        "      \n",
        "      return float(np.min(history.history[\"val_real_mae\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVBhvBwsFtWT"
      },
      "source": [
        "import neptune\n",
        "import neptunecontrib.monitoring.optuna as opt_utils\n",
        "from neptunecontrib.monitoring.keras import NeptuneMonitor\n",
        "\n",
        "neptune.init(api_token='ANONYMOUS', project_qualified_name='shared/optuna-integration')\n",
        "\n",
        "def main():\n",
        "\n",
        "    storage='sqlite:///APPNP_Optuna.db'\n",
        "    # Create experiment\n",
        "    neptune.create_experiment('optuna-sweep-advanced')\n",
        "    # Create callback to log advanced options during the sweep\n",
        "    neptune_callback = opt_utils.NeptuneCallback(log_study=True, log_charts=True)\n",
        "    study = optuna.create_study(direction=\"minimize\",storage=storage)\n",
        "    study.optimize(objective, timeout=3*3600,n_jobs=1,callbacks=[neptune_callback])\n",
        "    neptune.stop()\n",
        "    pruned_trials = [t for t in study.trials if t.state == optuna.structs.TrialState.PRUNED]\n",
        "    complete_trials = [t for t in study.trials if t.state == optuna.structs.TrialState.COMPLETE]\n",
        "    print(\"Study statistics: \")\n",
        "    print(\"  Number of finished trials: \", len(study.trials))\n",
        "    print(\"  Number of pruned trials: \", len(pruned_trials))\n",
        "    print(\"  Number of complete trials: \", len(complete_trials))\n",
        "\n",
        "    print(\"Best trial:\")\n",
        "    trial = study.best_trial\n",
        "\n",
        "    print(\"  Value: \", trial.value)\n",
        "\n",
        "    print(\"  Params: \")\n",
        "    for key, value in trial.params.items():\n",
        "        print(\"    {}: {}\".format(key, value))\n",
        "\n",
        "    return study,trial"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AB7tFoIcuOSP"
      },
      "source": [
        "study,trial=main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GN7h7o6-AVC_"
      },
      "source": [
        "## Ensemble"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24qrNorjZbyD"
      },
      "source": [
        "def appnp_model(lr,n_layers=4,tp=0.5,dropout=0.15,bn=True,residual=True,lookahead=True,optimizer='Novograd',activation='elu',scaled=False):\n",
        "\n",
        "      custom_appnp = APPNP(\n",
        "        layer_sizes=[200],\n",
        "        activations=[LR],\n",
        "        generator=generator,\n",
        "        teleport_probability=tp)\n",
        "\n",
        "      feature_layers = []\n",
        "\n",
        "      model = Sequential()\n",
        "\n",
        "      for i in range(n_layers):\n",
        "        model.add((ResDense(n_cols, activation=LR,dropout_rate=dropout,bn=bn)))\n",
        "      \n",
        "      model.add(Dense(100,activation=LR)) ## should be elu\n",
        "      model.add(Dropout(dropout))\n",
        "      if bn : model.add(BatchNormalization())\n",
        "      model.add(Dense(1,activation=LR)) ## should be elu\n",
        "\n",
        "\n",
        "\n",
        "      custom_appnp._feature_layers = [model]\n",
        "\n",
        "      x_inp, x_out = custom_appnp.in_out_tensors()\n",
        "\n",
        "      custom_appnp_model = tf.keras.models.Model(inputs=x_inp, outputs=x_out)\n",
        "      if optimizer == 'Novograd':opt = NovoGrad(lr=lr)\n",
        "      else : opt=tfa.optimizers.LAMB(learning_rate=lr)\n",
        "      if lookahead: opt = tfa.optimizers.Lookahead(opt) ##WRAP OPTIMIZER WITH LookAhead layer \n",
        "      custom_appnp_model.compile(loss=\"mae\",metrics=[real_mae],optimizer=opt)\n",
        "\n",
        "\n",
        "      return custom_appnp_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beMlrq38q8Qq"
      },
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "\"\"\" Tensorflow orginal early stopping call back does not restore best weights like it should \"\"\"\n",
        "\n",
        "class ReturnBestEarlyStopping(EarlyStopping):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(ReturnBestEarlyStopping, self).__init__(**kwargs)\n",
        "\n",
        "    def on_train_end(self, logs=None):\n",
        "        if self.stopped_epoch > 0:\n",
        "            if self.verbose > 0:\n",
        "                print(f'\\nEpoch {self.stopped_epoch + 1}: early stopping')\n",
        "        elif self.restore_best_weights:\n",
        "            if self.verbose > 0:\n",
        "                print('Restoring model weights from the end of the best epoch.')\n",
        "            self.model.set_weights(self.best_weights)\n",
        "\n",
        "n_cols = df_full_sc.shape[1]\n",
        "\n",
        "#params = {'n_layers': 4, 'tp': 0.9939236776191182, 'bn': True, 'dropout': 0.4026519014278893, 'lr': 0.02205957618270451, 'optimizer': 'LAMB', 'lookahead': False, 'activation': 'lr'}\n",
        "#params = {'n_layers': 6, 'tp': 0.7513095895530512, 'bn': False, 'dropout': 0.18498022320223975, 'lr': 0.07189637001413784, 'optimizer': 'Novograd', 'lookahead': True, 'activation': 'lr'}\n",
        "params = {'n_layers': 6, 'tp': 0.8954708163270214, 'bn': False, 'dropout': 0.5157525345254237, 'lr': 0.07301861599956465, 'optimizer': 'LAMB', 'lookahead': False, 'activation': 'relu'}\n",
        "custom_appnp_model = appnp_model(**params)\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_real_mae', factor=0.5,mode = \"min\",patience=75, min_lr=1e-6,min_delta=0.066,verbose=1)\n",
        "es = ReturnBestEarlyStopping(monitor='val_real_mae',restore_best_weights=True, patience=250,min_delta=0.066,verbose=1)\n",
        "\n",
        "\n",
        "history=custom_appnp_model.fit(train_gen,epochs=2500,validation_data=val_gen,shuffle=False,callbacks =[reduce_lr,es])  # this should be False, since shuffling data means shuffling the whole graph                                     \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "tyILncbhgh4N",
        "outputId": "425e6312-c41f-47e5-81d0-7c8eb9f4a365"
      },
      "source": [
        "\"\"\" Verify model predictions\n",
        "\n",
        "Loss on validation data isn't the same as tensorflow ...\"\"\"\"\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error as mae\n",
        "\n",
        "\n",
        "float(np.min(history.history[\"val_real_mae\"]))\n",
        "y_val = custom_appnp_model.predict(val_gen)\n",
        "diff = y_val.reshape(-1)-val_targets.values.reshape(-1)\n",
        "diff2 = np.round(y_val.reshape(-1))-val_targets.values.reshape(-1)\n",
        "z=plt.hist(diff,bins=100)\n",
        "\n",
        "mae(np.round(y_val.reshape(-1)),val_targets.values.reshape(-1))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.5246327"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4NiFZKRgQ35"
      },
      "source": [
        "\"\"\" Write predictions into file \"\"\"\n",
        "\n",
        "y_pred = custom_appnp_model.predict(test_gen)\n",
        "# write the predictions to file\n",
        "test_data['h_index_pred'].update(pd.Series(np.round_(y_pred.reshape(-1))))\n",
        "test_data.loc[:,[\"authorID\",\"h_index_pred\"]].to_csv('test_predictions.csv', index=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xOD7d4nykgJ"
      },
      "source": [
        "# Bagging ensemble (sounds cool doesn't work)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXuci4hfNdUl"
      },
      "source": [
        "zfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
        "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
        "\n",
        "\n",
        "class EnsembleModel(BaseEstimator):\n",
        "    def __init__(self, n_folds=5):\n",
        "        #self.meta_model = meta_model\n",
        "        self.n_folds = n_folds\n",
        "        self.params = params\n",
        "   \n",
        "    # We again fit the data on clones of the original models\n",
        "    def fit(self,generator,train_data):\n",
        "        n_cols = df_full_sc.shape[1]\n",
        "        self.trained_models = []\n",
        "        \n",
        "\n",
        "        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n",
        "\n",
        "        self.model_losses = []\n",
        "  \n",
        "        for train, val in kfold.split(train_data.index):\n",
        "            train_index,val_index = train_data.index.values[train],train_data.index.values[val]\n",
        "            instance = appnp_model(**self.params)\n",
        "            self.trained_models.append(instance)\n",
        "            train_generator = generator.flow(train_index, train_data.loc[train_index])\n",
        "            val_generator = generator.flow(val_index, train_data.loc[val_index])\n",
        "            history = instance.fit(train_generator,validation_data = val_generator,epochs=2000,shuffle=False,callbacks=callbacks())    \n",
        "            self.model_losses.append(float(np.min(history.history[\"val_real_mae\"])))\n",
        "\n",
        "        \n",
        "        return self\n",
        "   \n",
        "        \n",
        "    def predict(self, test_generator):\n",
        "      \n",
        "      return np.vstack([model.predict(test_generator) for model in self.trained_models]).reshape(self.n_folds,-1).mean(axis=0)\n",
        "\n",
        "\n",
        "\n",
        "ensemble = EnsembleModel(n_folds=6)\n",
        "ensemble.fit(generator,train_data)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}